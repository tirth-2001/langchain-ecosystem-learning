# üß† 7.1.2 ‚Äî Single Node Graph (LLM ‚Üí Output)

---

## üéØ Goal

Create a **LangGraph with a single LLM node**, where:

```
State Input ‚Üí LLM Node ‚Üí Output
```

We‚Äôll:

- Add one node that uses an OpenAI model.
- Pass input state (`query`) ‚Üí LLM ‚Üí response (`answer`).
- See how LangGraph handles async LLM operations.

---

## üß© Conceptual Overview

LangGraph doesn‚Äôt ‚Äúthink‚Äù by itself ‚Äî it orchestrates **nodes**.
Each node can be:

- A function (like earlier)
- A **LangChain Runnable** (LLM, chain, tool, etc.)

---

### üß† Mental Model

```
Input State {
  query: "What's the capital of France?"
}

‚Üì (passed to node)

[ Node: LLM ]
  prompt = "Answer concisely: {query}"
  output = "The capital of France is Paris."

‚Üì
Final State {
  query: "What's the capital of France?",
  answer: "The capital of France is Paris."
}
```

---

## ‚öôÔ∏è File: `src/langgraph/7.1.2-single-llm-node.ts`

> Uses OpenAI model, Annotation-based API, and a single LLM node.

```ts
import { StateGraph, Annotation, END } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import dotenv from 'dotenv'
dotenv.config()

/**
 * 1Ô∏è‚É£ Define State Annotation
 */
const StateAnnotation = Annotation.Root({
  query: Annotation<string>({ reducer: (x) => x }), // input
  answer: Annotation<string>({ reducer: (x) => x }), // output
})

/**
 * 2Ô∏è‚É£ Build a simple LLM Runnable (LangChain chain)
 * RunnableSequence = prompt ‚Üí model ‚Üí output mapping
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini', // efficient, low-cost
  temperature: 0.3,
})

const prompt = ChatPromptTemplate.fromTemplate(`Answer the user's question clearly and concisely:\n\nQuestion: {query}`)

const chain = RunnableSequence.from([
  prompt,
  model,
  // Map model output ‚Üí graph state key "answer"
  (response) => ({
    answer: response.content,
  }),
])

/**
 * 3Ô∏è‚É£ Define the LLM Node
 */
async function llmNode(state: typeof StateAnnotation.State) {
  const result = await chain.invoke({ query: state.query })
  return { answer: result.answer }
}

/**
 * 4Ô∏è‚É£ Create the Graph
 */
const graph = new StateGraph(StateAnnotation)
  .addNode('llmResponder', llmNode)
  .addEdge('__start__', 'llmResponder')
  .addEdge('llmResponder', END)

const app = graph.compile()

/**
 * 5Ô∏è‚É£ Run it
 */
async function main() {
  const input = { query: "What's the capital of France?" }
  const result = await app.invoke(input)
  console.log('‚úÖ Final State:', result)
}

main()
```

---

## ‚ñ∂Ô∏è Run

```bash
npx ts-node src/langgraph/7.1.2-single-llm-node.ts
```

---

## ‚úÖ Expected Output

```bash
‚úÖ Final State: {
  query: "What's the capital of France?",
  answer: "The capital of France is Paris."
}
```

(Exact text may vary slightly based on LLM temperature.)

---

## üß† Key Takeaways

| Concept                      | What You Learned                            |
| ---------------------------- | ------------------------------------------- |
| `Annotation.Root`            | Defines the graph‚Äôs data schema             |
| `RunnableSequence`           | LangChain-compatible chain inside LangGraph |
| Node returns partial state   | The return object merges into current state |
| `__start__` ‚Üí Node ‚Üí END     | The simplest valid graph flow               |
| LLM can be wrapped as a node | Each node can itself be a mini chain        |

---

## ‚ö° Why This Matters

You just built:

- A **deterministic**, inspectable workflow
- With **LLM reasoning** as one node
- That can later be combined with:

  - Memory nodes
  - Tool nodes
  - Branching logic
  - Graph checkpoints

Essentially ‚Äî you‚Äôve built a **stateful single-step agent**, the smallest possible LangGraph-based AI system.

---

## üßæ Quick Recap Notes

- ‚úÖ Use `Annotation.Root()` ‚Äî no deprecated syntax
- ‚úÖ Always define entry: `.addEdge("__start__", "nodeName")`
- ‚úÖ Return `{ key: value }` from node to mutate state
- ‚úÖ Use LangChain‚Äôs `RunnableSequence` to plug any LLM logic
- ‚úÖ The compiled app (`app.invoke()`) runs like a regular chain
