# ğŸ§  **7.1.4 â€” Memory Node: Persist Short-Term Context (ChatMemoryGraph)**

---

## ğŸ¯ **Goal**

Build a graph that remembers previous user inputs and model replies:

```
User Input #1 â†’ LLM â†’ Answer
User Input #2 â†’ LLM (with memory of #1) â†’ Answer 2
```

All within a single **LangGraph state machine**.
Weâ€™ll use LangChainâ€™s `BufferMemory` or message history logic inside a graph node.

---

## ğŸ§© **Conceptual Overview**

In a normal chain or agent, `BufferMemory` holds chat messages like:

```
Human: Hi
AI: Hello! How can I help?
Human: Tell me about AI.
```

Weâ€™ll bring that concept into LangGraph by:

- Using an **annotation for chat history**.
- Adding a **Memory Node** that updates and reads that history.
- Feeding it into the **LLM Node** for contextual replies.

---

## ğŸ§  **Visual Flow**

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  chatHistoryâ”‚  (array of messages)
 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
[ Memory Node ] â†’ updates history
       â†“
[ LLM Node ] â†’ generates contextual reply
       â†“
END
```

---

## âš™ï¸ **Code Implementation**

### ğŸ“ File: `src/langgraph/7.1.4-memory-node.ts`

```ts
import { StateGraph, Annotation, END } from '@langchain/langgraph'
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { ChatOpenAI } from '@langchain/openai'
import type { BaseMessage } from '@langchain/core/messages'
import dotenv from 'dotenv'
dotenv.config()

/**
 * 1ï¸âƒ£ Define State Annotations
 */
const StateAnnotation = Annotation.Root({
  userInput: Annotation<string>({ reducer: (current, update) => update }),
  chatHistory: Annotation<BaseMessage[]>({
    // merge previous messages with new ones
    reducer: (current = [], update = []) => [...current, ...update],
  }),
  answer: Annotation<string>({ reducer: (current, update) => update }),
})

/**
 * 2ï¸âƒ£ Memory Node â€” manages message history
 */
import { HumanMessage, AIMessage } from '@langchain/core/messages'

async function memoryNode(state: typeof StateAnnotation.State) {
  // Simulate message storage (this could be Redis, DB, etc.)
  const newHistory = [new HumanMessage(state.userInput), ...(state.answer ? [new AIMessage(state.answer)] : [])]
  return { chatHistory: newHistory }
}

/**
 * 3ï¸âƒ£ LLM Node â€” context-aware chat generation
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.3,
})

const prompt = ChatPromptTemplate.fromMessages([
  ['system', 'You are a helpful AI that remembers prior context.'],
  new MessagesPlaceholder('chatHistory'), // pull memory into context
  ['human', '{userInput}'],
])

const chain = RunnableSequence.from([prompt, model, (output) => ({ answer: output.content })])

async function llmNode(state: typeof StateAnnotation.State) {
  const result = await chain.invoke({
    userInput: state.userInput,
    chatHistory: state.chatHistory,
  })
  return { answer: result.answer }
}

/**
 * 4ï¸âƒ£ Build the Graph
 */
const graph = new StateGraph(StateAnnotation)
  .addNode('memory', memoryNode)
  .addNode('llmResponder', llmNode)
  .addEdge('__start__', 'memory')
  .addEdge('memory', 'llmResponder')
  .addEdge('llmResponder', END)

const app = graph.compile()

/**
 * 5ï¸âƒ£ Run Multiple Times to Simulate Persistent Memory
 */
async function main() {
  let state = { userInput: 'Hello! Who are you?' }
  state = await app.invoke(state)
  console.log('\nğŸ§  Run 1:', state.answer)

  // Simulate new user input â€” graph retains context in chatHistory
  state = { ...state, userInput: 'Can you remind me what I just asked?' }
  const result2 = await app.invoke(state)

  console.log('\nğŸ§  Run 2:', result2.answer)
}

main()
```

---

## â–¶ï¸ **Run Command**

```bash
npx ts-node src/langgraph/7.1.4-memory-node.ts
```

---

## âœ… **Expected Output**

```
ğŸ§  Run 1: Hello! Iâ€™m an AI assistant here to help.
ğŸ§  Run 2: You asked me who I am. Iâ€™m an AI assistant!
```

(Exact phrasing may differ slightly, but the key is **memory recall**.)

---

## ğŸ” **How It Works**

| Step | Node        | Role                                             | Output                  |
| ---- | ----------- | ------------------------------------------------ | ----------------------- |
| 1ï¸âƒ£   | Memory Node | Appends user+AI messages to chatHistory          | `[Human, AI, Human, â€¦]` |
| 2ï¸âƒ£   | LLM Node    | Uses chatHistory + userInput â†’ contextual answer | `answer`                |
| ğŸ§©   | StateGraph  | Passes and merges state automatically            | Persistent memory       |

---

## âš™ï¸ **Behind the Scenes**

LangGraph doesnâ€™t have â€œbuilt-in memoryâ€ â€” itâ€™s **just state**.
So the reducer `(current, update) => [...current, ...update]` acts as your memory appender.

You can replace this with:

- `Redis` (for long sessions)
- `MongoDB` (for persistent context)
- `BufferWindow` reducer (for recent context only)

This means your graph can support:

- Chat history persistence
- State replay or resume
- Contextual tool calling later (Stage 7.4)

---

## ğŸ§  **Key Concepts Learned**

| Concept                              | Meaning                                        |
| ------------------------------------ | ---------------------------------------------- |
| `Annotation<BaseMessage[]>`          | You can track an array of messages in state    |
| Reducer merging                      | Controls how messages accumulate               |
| `MessagesPlaceholder("chatHistory")` | Dynamically injects prior messages into prompt |
| Multi-run graph state                | Allows persistence between invocations         |

---

## ğŸ§¾ **Quick Revision Notes**

- Use reducers for state accumulation (`current + update`)
- Memory = just state evolution, not a separate module
- Inject memory via `MessagesPlaceholder`
- Each graph invocation can â€œrememberâ€ previous context
- Perfect base for chatbots, agents, or planners
