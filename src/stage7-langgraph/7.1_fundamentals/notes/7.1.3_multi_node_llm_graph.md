# âš™ï¸ 7.1.3 â€” Multi-Node Flow (Input â†’ LLM â†’ Post-Processor Node)

---

## ğŸ¯ **Goal**

Weâ€™ll construct a two-step graph:

```
User Input
   â†“
[ LLM Node ] â†’ generates answer
   â†“
[ Formatter Node ] â†’ cleans & styles answer
   â†“
Output
```

---

## ğŸ§  **Conceptual Overview**

| Stage              | Purpose                                                                                     |
| ------------------ | ------------------------------------------------------------------------------------------- |
| **LLM Node**       | Uses LangChain chain to generate the base answer                                            |
| **Formatter Node** | Post-processes the LLM output â€” e.g., title-casing, markdown wrapping, or bullet conversion |
| **State Flow**     | Output of one node becomes input for next node via LangGraphâ€™s state mutation               |

---

## ğŸ§© **Visual Flow**

```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   query (input)      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        [ LLM Node ]
               â†“
        [ Formatter Node ]
               â†“
          { answer }
```

---

## ğŸ§° **Code Implementation**

### ğŸ“ File: `src/langgraph/7.1.3-multi-node-flow.ts`

```ts
import { StateGraph, Annotation, END } from '@langchain/langgraph'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { ChatOpenAI } from '@langchain/openai'
import dotenv from 'dotenv'
dotenv.config()

/**
 * 1ï¸âƒ£ Define State Annotation
 */
const StateAnnotation = Annotation.Root({
  query: Annotation<string>({
    reducer: (current, update) => update,
  }),
  rawAnswer: Annotation<string>({
    reducer: (current, update) => update,
  }),
  formattedAnswer: Annotation<string>({
    reducer: (current, update) => update,
  }),
})

/**
 * 2ï¸âƒ£ Node 1 â†’ LLM Node
 * Takes query â†’ returns rawAnswer
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.3,
})

const llmPrompt = ChatPromptTemplate.fromTemplate(`Answer briefly and informatively:\n\nQuestion: {query}`)

const llmChain = RunnableSequence.from([llmPrompt, model, (output) => ({ rawAnswer: output.content })])

async function llmNode(state: typeof StateAnnotation.State) {
  const result = await llmChain.invoke({ query: state.query })
  return { rawAnswer: result.rawAnswer }
}

/**
 * 3ï¸âƒ£ Node 2 â†’ Formatter Node
 * Takes rawAnswer â†’ formattedAnswer
 */
async function formatterNode(state: typeof StateAnnotation.State) {
  const text = state.rawAnswer.trim()

  // Example: Add markdown + clean formatting
  const formatted = `### ğŸ¤– AI Response\n${text[0].toUpperCase() + text.slice(1)}.`

  return { formattedAnswer: formatted }
}

/**
 * 4ï¸âƒ£ Build Graph
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('generate', llmNode)
  .addNode('format', formatterNode)
  .addEdge('__start__', 'generate') // Entry point
  .addEdge('generate', 'format') // Chain nodes
  .addEdge('format', END) // End node

const app = workflow.compile()

/**
 * 5ï¸âƒ£ Run the graph
 */
async function main() {
  const input = { query: 'What are the benefits of drinking green tea?' }
  const result = await app.invoke(input)

  console.log('\nâœ… Final State:')
  console.log(result)

  console.log('\nğŸ’¬ Formatted Output:\n', result.formattedAnswer)
}

main()
```

---

## â–¶ï¸ **Run Command**

```bash
npx ts-node src/langgraph/7.1.3-multi-node-flow.ts
```

---

## âœ… **Expected Output**

```
âœ… Final State:
{
  query: "What are the benefits of drinking green tea?",
  rawAnswer: "Green tea improves brain function, boosts metabolism, and contains antioxidants.",
  formattedAnswer: "### ğŸ¤– AI Response\nGreen tea improves brain function, boosts metabolism, and contains antioxidants."
}

ğŸ’¬ Formatted Output:
### ğŸ¤– AI Response
Green tea improves brain function, boosts metabolism, and contains antioxidants.
```

---

## ğŸ” **How It Works**

| Step | Node                      | Input        | Output               | Purpose                 |
| ---- | ------------------------- | ------------ | -------------------- | ----------------------- |
| 1ï¸âƒ£   | LLM Node (`generate`)     | query        | rawAnswer            | Get main content        |
| 2ï¸âƒ£   | Formatter Node (`format`) | rawAnswer    | formattedAnswer      | Format for UI / display |
| ğŸ§©   | Graph Engine              | Tracks state | merges between nodes | Orchestrates flow       |

---

## âš¡ **Why This Is Powerful**

- Nodes are **independent functions**
- Each node works on **typed state**
- Graph merges outputs using **reducers**
- You can **insert or replace nodes** later (e.g., memory node, RAG node)
- Behaves like a pipeline â€” but fully inspectable and resumable

---

## ğŸ§  **Real-World Extensions**

- ğŸª„ Add a _sentiment analyzer node_ next
- ğŸ’¬ Add a _translator node_ after formatter
- ğŸ§® Or make a _tool node_ to fetch data before LLM

---

## ğŸ§¾ **Quick Revision Notes**

- Multiple nodes = sequential flow via edges
- Each node can return **partial state updates**
- Reducers merge those into global state
- Start edge (`__start__`) defines entry
- `END` defines termination
- Nodes can be chained, branched, or looped
