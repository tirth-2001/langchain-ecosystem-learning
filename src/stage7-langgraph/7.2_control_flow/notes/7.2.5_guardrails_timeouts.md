# âœ… \*\*7.2.5 â€” Timeout & Guardrails

(Safe Graph Execution: Timeouts, Circuit Breakers, Fallback Nodes)\*\*

---

# âœ… **1. GOAL**

You will build a graph that:

### âœ… Cancels long-running nodes using **timeout wrappers**

### âœ… Enforces **maximum retries**

### âœ… Falls back to a safe response on failure

### âœ… Prevents infinite loops

### âœ… Ensures the graph always reaches a clean END node

This is how production systems operate:

- Donâ€™t wait forever for an LLM
- Avoid crash loops
- Provide safe backup responses
- Ensure graceful failure

---

# âœ… **2. THEORY (Concepts You Need)**

## ğŸ“Œ 2.1 Timeout Concepts in LangGraph

LangGraph does **not** provide a built-in â€œtimeout wrapper,â€ but you can implement your own using:

### âœ… A Promise.race wrapper

```ts
function withTimeout(promise, ms) {
  return Promise.race([promise, new Promise((_, reject) => setTimeout(() => reject(new Error('TIMEOUT')), ms))])
}
```

This gives us:

- Hard timeout limits (e.g., 2s, 5s)
- Immediate fallback logic

---

## ğŸ“Œ 2.2 Guardrails for Safety

Common safety guardrails:

| Guardrail Type      | Purpose                           |
| ------------------- | --------------------------------- |
| **Timeout**         | Node must respond in time         |
| **Retry**           | Retry temporary failures          |
| **Circuit Breaker** | After repeated failure â†’ fallback |
| **Fallback Node**   | Safe deterministic output         |
| **Max Loop Count**  | Prevent runaway loops             |

---

## ğŸ“Œ 2.3 Conditional Edges for Failure Routing

LangGraph supports:

```ts
.addConditionalEdges("someNode", (state) => {
  if (state.error) return "fallback"
  return "next"
}, {
  next: "normalPath",
  fallback: "fallbackNode"
})
```

This is how we route failures to safe outputs.

---

# âœ… **3. CODE SNIPPETS (Step-by-Step)**

We will build this graph:

```
Input
   â”‚
   â–¼
 riskyLLMNode (with timeout & retry)
   â”‚ OK                     â”‚ FAIL/TIMEOUT
   â–¼                        â–¼
 successNode           fallbackNode
   â”‚                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  mergeNode  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                        END
```

---

### âœ… 3.1 Timeout wrapper

```ts
function withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) => setTimeout(() => reject(new Error('TIMEOUT_EXCEEDED')), ms)),
  ])
}
```

---

### âœ… 3.2 Retry wrapper (same pattern as previous stage)

```ts
async function retry<T>(fn: () => Promise<T>, attempts = 2, delay = 300): Promise<T> {
  let error: any
  for (let i = 0; i < attempts; i++) {
    try {
      return await fn()
    } catch (err) {
      error = err
      if (i < attempts - 1) {
        await new Promise((r) => setTimeout(r, delay))
      }
    }
  }
  throw error
}
```

---

### âœ… 3.3 Risky LLM node (may hang or fail)

```ts
async function riskyLLMNode(state) {
  console.log('ğŸ˜¬ Running riskyLLMNodeâ€¦')

  const run = async () => {
    const output = await llm.invoke([['user', `Respond within 20 words: ${state.input}`]])
    return output.content.trim()
  }

  try {
    const result = await withTimeout(
      retry(run, 2), // Try twice
      1500, // Hard timeout: 1.5s
    )

    return {
      ok: true,
      result,
    }
  } catch (err) {
    console.log('âŒ riskyLLMNode failed:', err.message)
    return {
      ok: false,
      error: err.message,
    }
  }
}
```

---

### âœ… 3.4 Success node

```ts
async function successNode(state) {
  console.log('âœ… successNode reached')
  return {
    final: `âœ… SUCCESS: ${state.result}`,
  }
}
```

---

### âœ… 3.5 Fallback node

```ts
async function fallbackNode(state) {
  console.log('âš ï¸ fallbackNode triggered')
  return {
    final: `âš ï¸ Fallback: Model unavailable. Reason: ${state.error}`,
  }
}
```

---

### âœ… 3.6 Merge node (both paths converge)

```ts
async function mergeNode(state) {
  return state
}
```

---

### âœ… 3.7 Conditional Routing

```ts
.addConditionalEdges(
  "riskyLLMNode",
  (state) => (state.ok ? "ok" : "fail"),
  {
    ok: "successNode",
    fail: "fallbackNode",
  }
)
```

---

# âœ… **4. FINAL FULL FILE (Plug-and-Play)**

### âœ… **ğŸ“„ `timeouts-guardrails.ts`**

```ts
import { Annotation, StateGraph, START, END } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import 'dotenv/config'

/**
 * 1) State model
 */
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  ok: Annotation<boolean>(),
  result: Annotation<string>(),
  error: Annotation<string>(),
  final: Annotation<string>(),
})

/**
 * 2) Helpers
 */
function withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) => setTimeout(() => reject(new Error('TIMEOUT_EXCEEDED')), ms)),
  ])
}

async function retry<T>(fn: () => Promise<T>, attempts = 2, delay = 300): Promise<T> {
  let lastError: any
  for (let i = 0; i < attempts; i++) {
    try {
      return await fn()
    } catch (e) {
      lastError = e
      if (i < attempts - 1) {
        await new Promise((r) => setTimeout(r, delay))
      }
    }
  }
  throw lastError
}

/**
 * 3) Nodes
 */
const llm = new ChatOpenAI({ modelName: 'gpt-4o-mini', temperature: 0.4 })

async function riskyLLMNode(state: typeof StateAnnotation.State) {
  console.log('ğŸ˜¬ riskyLLMNode runningâ€¦')

  const runModel = async () => {
    const out = await llm.invoke([['user', `Respond in 20 words: ${state.input}`]])
    return out.content.trim()
  }

  try {
    const result = await withTimeout(
      retry(runModel, 2),
      1500, // 1.5s timeout
    )

    return { ok: true, result }
  } catch (err: any) {
    console.log('âŒ riskyLLMNode failed:', err.message)
    return { ok: false, error: err.message }
  }
}

async function successNode(state) {
  console.log('âœ… Success path')
  return { final: `âœ… Success: ${state.result}` }
}

async function fallbackNode(state) {
  console.log('âš ï¸ Fallback path')
  return { final: `âš ï¸ Fallback: ${state.error}` }
}

async function mergeNode(state) {
  return state
}

/**
 * 4) Graph
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('riskyLLMNode', riskyLLMNode)
  .addNode('successNode', successNode)
  .addNode('fallbackNode', fallbackNode)
  .addNode('mergeNode', mergeNode)
  .addEdge(START, 'riskyLLMNode')
  .addConditionalEdges('riskyLLMNode', (state) => (state.ok ? 'ok' : 'fail'), {
    ok: 'successNode',
    fail: 'fallbackNode',
  })
  .addEdge('successNode', 'mergeNode')
  .addEdge('fallbackNode', 'mergeNode')
  .addEdge('mergeNode', END)

const app = workflow.compile()

/**
 * 5) Demo
 */
async function main() {
  console.log('\n=== 7.2.5 Timeout & Guardrail Demo ===\n')

  const result = await app.invoke({
    input: 'Explain quantum gravity in one sentence.',
  })

  console.log('\nâœ… Final Output:', result.final)
}

main().catch(console.error)
```

---

# âœ… **5. Notes & Tips (Production-Level Advice)**

### âœ… Use timeouts when:

- You call external APIs (SerpAPI, weather API, etc.)
- LLM streaming may stall
- You have multiple branches and one must not block the others

---

### âœ… Use fallback nodes to ensure:

- No request ever ends in error
- Every execution path reaches END
- User always gets a response

---

### âœ… Always track errors explicitly:

```ts
error: Annotation<string>()
```

---

### âœ… Combine with parallel graphs:

Timeouts work beautifully with the previous section:

```
summarizeNode (fast LLM)
keywordNode (slow/hanging LLM) â†’ timeout â†’ fallback
```

---

### âœ… In real systems, guardrails prevent:

- Infinite loops
- Deadlocks
- LLM hallucinated tool calls
- Slow APIs blocking the UX
- Costly runaway calls
