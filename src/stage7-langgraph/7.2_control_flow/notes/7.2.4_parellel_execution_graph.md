# âœ… **7.2.4 â€” Parallel Execution (Run LLM Nodes Concurrently & Merge Results)**

**Pattern:** Goal â†’ Theory â†’ Code Snippets â†’ Final File â†’ Notes

---

# âœ… **1. GOAL**

Build a LangGraph workflow where:

```
Input Text
    â”œâ”€â”€â–¶ Summarize (LLM)
    â””â”€â”€â–¶ Extract Keywords (LLM)
                â–¼
              Join Node
                â–¼
                END
```

âœ… Two branches run **in parallel**
âœ… Both write into the shared state
âœ… A final "join node" merges their results

This is the **foundation** for:

- Multi-tool agent pipelines
- Multi-model fusion
- Hybrid RAG (summary + search)
- Parallel tool execution in AutoGPT-style planners

---

# âœ… **2. THEORY (Concepts You Need)**

### **ðŸ“Œ 2.1 How LangGraph Parallelism Works**

LangGraph doesn't use threads â€” instead:

- If **a node returns an array of next node names**, LangGraph runs them _as parallel branches_.
- Each branch updates the shared state via **reducers**.
- The graph automatically tracks dependencies:

  - A node only runs when _all_ inbound parents have completed.

Example:

```ts
return ['nodeA', 'nodeB'] // <-- parallel fan-out
```

---

### **ðŸ“Œ 2.2 Reducers Are Critical**

Because multiple nodes write to state concurrently, we need **merge reducers** like:

```ts
keywords: Annotation<string[]>({
  reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
})
```

Without good reducers:

- One node may overwrite the otherâ€™s output
- Final state will be incomplete

---

### **ðŸ“Œ 2.3 Join Node**

A node with inbound edges from multiple nodes becomes a **join node**:

```
summarizeNode â†’
                 â†’ joinNode â†’ END
keywordsNode  â†’
```

LangGraph ensures:
âœ… joinNode runs only after both branches have completed
âœ… You don't need manual synchronization

---

# âœ… **3. CODE SNIPPETS (Step-by-Step)**

---

### âœ… **3.1 State Model**

Reducers ensure both branches write safely.

```ts
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  summary: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
  keywords: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  combined: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
})
```

---

### âœ… **3.2 Base Models & Chains**

**Summary LLM chain:**

```ts
const summarizePrompt = ChatPromptTemplate.fromTemplate(`Summarize the following text in one crisp sentence:\n\n{text}`)
const summarizeChain = RunnableSequence.from([summarizePrompt, model, (out) => out.content.trim()])
```

**Keyword extractor:**

```ts
const keywordsPrompt = ChatPromptTemplate.fromTemplate(
  `Extract 5-8 concise keywords from the text. Output as JSON array of strings.`,
)
const keywordsChain = RunnableSequence.from([
  keywordsPrompt,
  model,
  (out) => {
    try {
      return JSON.parse(out.content.trim())
    } catch {
      return [out.content.trim()]
    }
  },
])
```

---

### âœ… **3.3 Nodes**

**Fan-out node (parallel branching):**

```ts
function fanOutNode() {
  return ['summarizeNode', 'keywordsNode'] as const
}
```

**Summarize branch:**

```ts
async function summarizeNode(state) {
  const summary = await summarizeChain.invoke({ text: state.input })
  return { summary }
}
```

**Keywords branch:**

```ts
async function keywordsNode(state) {
  const kw = await keywordsChain.invoke({ text: state.input })
  return { keywords: kw }
}
```

**Join node:**

```ts
async function joinNode(state) {
  return {
    combined: `Summary: ${state.summary}\nKeywords: ${state.keywords.join(', ')}`,
  }
}
```

---

# âœ… **4. FINAL FILE (Copyâ€“Paste, Run Immediately)**

âœ… Fully runnable
âœ… Includes START â†’ parallel â†’ join â†’ END
âœ… Uses `gpt-4o-mini`

---

### âœ… **ðŸ“„ `parallel-graph.ts`**

```ts
import { Annotation, StateGraph, END, START } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import 'dotenv/config'

/**
 * 1) State model
 */
const StateAnnotation = Annotation.Root({
  input: Annotation<string>({}),
  summary: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
  keywords: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  combined: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
})

/**
 * 2) LLM setup
 */
const model = new ChatOpenAI({ modelName: 'gpt-4o-mini', temperature: 0.2 })

const summarizePrompt = ChatPromptTemplate.fromTemplate(
  `Summarize the following text in one concise sentence:\n\n{text}`,
)
const summarizeChain = RunnableSequence.from([summarizePrompt, model, (out) => out.content.trim()])

const keywordsPrompt = ChatPromptTemplate.fromTemplate(
  `Extract 5-8 keywords as a JSON array of strings.\n\nText:\n{text}`,
)
const keywordsChain = RunnableSequence.from([
  keywordsPrompt,
  model,
  (out) => {
    try {
      return JSON.parse(out.content.trim())
    } catch {
      return [out.content.trim()]
    }
  },
])

/**
 * 3) Nodes
 */
function fanOutNode() {
  return ['summarizeNode', 'keywordsNode'] as const
}

async function summarizeNode(state: typeof StateAnnotation.State) {
  console.log('ðŸ“ summarizeNode runningâ€¦')
  const summary = await summarizeChain.invoke({ text: state.input })
  return { summary }
}

async function keywordsNode(state: typeof StateAnnotation.State) {
  console.log('ðŸ”Ž keywordsNode runningâ€¦')
  const kw = await keywordsChain.invoke({ text: state.input })
  return { keywords: kw }
}

async function joinNode(state: typeof StateAnnotation.State) {
  console.log('ðŸ¤ joinNode combining resultsâ€¦')
  return {
    combined: `Summary: ${state.summary}\nKeywords: ${state.keywords.join(', ')}`,
  }
}

/**
 * 4) Build graph
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('fanOut', fanOutNode)
  .addNode('summarizeNode', summarizeNode)
  .addNode('keywordsNode', keywordsNode)
  .addNode('join', joinNode)
  .addEdge(START, 'fanOut')
  .addEdge('summarizeNode', 'join')
  .addEdge('keywordsNode', 'join')
  .addEdge('join', END)

const app = workflow.compile()

/**
 * 5) Demo
 */
async function main() {
  console.log('\n=== 7.2.4 Parallel Execution Demo ===\n')

  const text =
    'LangGraph enables building deterministic, inspectable AI workflows with nodes, edges, memory, and control flow.'

  const result = await app.invoke({ input: text })

  console.log('\nâœ… FINAL STATE:')
  console.log({
    summary: result.summary,
    keywords: result.keywords,
    combined: result.combined,
  })
}

main().catch(console.error)
```

---

# âœ… **5. Notes & Tips**

### âœ… Parallel branches are triggered by:

```ts
return ['nodeA', 'nodeB']
```

### âœ… Joins are automatic:

The join node is executed **after both parents complete**.

### âœ… Reducers matter:

Especially for arrays:

```ts
reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])]
```

### âœ… Parallel execution is ideal for:

- Hybrid RAG (semantic + keyword search)
- Multi-tool agent reasoning
- Model fusion (Claude + GPT)
- Multi-expert pipelines
- Summaries + insights + metadata extraction
