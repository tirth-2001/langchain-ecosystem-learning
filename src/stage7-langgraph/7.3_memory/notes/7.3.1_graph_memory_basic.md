# âœ… **7.3.1 â€” Graph Memory Basics (State Store + Recall Node)**

---

## ğŸ¥… **1. GOAL**

In this module, youâ€™ll learn to add **memory and recall capability** into a LangGraph workflow.

You will build a small conversational graph where the LLM:

- âœ… **Remembers** past messages (stored in state)
- âœ… **Recalls** that memory on the next input
- âœ… **Uses reducers** to persist message history
- âœ… **Simulates multi-turn conversation context**

---

## ğŸ§  **2. THEORY**

### ğŸ“˜ 2.1 What is Memory in LangGraph?

Unlike plain LangChain chains, LangGraphâ€™s state **is the memory** â€” persisted across runs or within a session.

Memory =
ğŸ‘‰ **State fields** that accumulate, not overwrite.
ğŸ‘‰ **Reducers** control how updates merge.

Example:

```ts
chatHistory: Annotation<string[]>({
  reducer: (current, update) => [...(current ?? []), ...(update ?? [])],
})
```

---

### ğŸ“˜ 2.2 Common Memory Patterns

| Type                 | Example                       | Description                          |
| -------------------- | ----------------------------- | ------------------------------------ |
| **BufferMemory**     | `[Human, AI, Human, AI]`      | Stores conversation history linearly |
| **SummaryMemory**    | â€œUser asked about X earlier.â€ | Summarizes older turns               |
| **PersistentMemory** | MongoDB, Redis                | Stores checkpoints between runs      |

In LangGraph, we control this using **state annotations + reducers + memory nodes**.

---

### ğŸ“˜ 2.3 Flow Youâ€™ll Build

```
Input
  â†“
MemoryNode (adds user input to chat history)
  â†“
LLMNode (uses chat history to respond)
  â†“
MemoryNode (adds LLM reply back to chat history)
  â†“
END
```

Output:
ğŸ§  â€œConversational memory graphâ€
â€” retains history across multiple invocations in the same session.

---

## âš™ï¸ **3. CODE SNIPPETS (Step-by-Step)**

---

### âœ… 3.1 Define State with Memory

```ts
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  response: Annotation<string>(),
})
```

---

### âœ… 3.2 Create a Chat Model

```ts
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.5,
})
```

---

### âœ… 3.3 Create LLM Chain with Context

Weâ€™ll dynamically insert the full chat history in the prompt.

```ts
const chatPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant. Use conversation history to reply naturally.

Chat history:
{chatHistory}

User: {input}
AI:
`)

const chatChain = RunnableSequence.from([chatPrompt, model, (out) => out.content.trim()])
```

---

### âœ… 3.4 Create Memory Nodes

**MemoryNode (store user input)**

```ts
async function memoryNodeInput(state) {
  console.log('ğŸ§  Saving user input to chat history...')
  return { chatHistory: [`Human: ${state.input}`] }
}
```

**LLM Node (generate reply)**

```ts
async function llmNode(state) {
  console.log('ğŸ’¬ LLM generating reply...')
  const historyText = (state.chatHistory ?? []).join('\n')
  const response = await chatChain.invoke({
    input: state.input,
    chatHistory: historyText,
  })
  return { response }
}
```

**MemoryNode (store LLM output)**

```ts
async function memoryNodeOutput(state) {
  console.log('ğŸ§  Storing assistant reply...')
  return { chatHistory: [`AI: ${state.response}`] }
}
```

---

# ğŸ’» **4. FINAL FILE (Plug-and-Play Code)**

> ğŸ“„ `graph-memory-basic.ts`

```ts
import { Annotation, StateGraph, START, END } from '@langchain/langgraph'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { ChatOpenAI } from '@langchain/openai'
import { RunnableSequence } from '@langchain/core/runnables'
import 'dotenv/config'

/**
 * 1ï¸âƒ£ State model with reducer-based memory
 */
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  response: Annotation<string>(),
})

/**
 * 2ï¸âƒ£ LLM setup
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.5,
})

const chatPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant. Use conversation history to reply naturally.

Chat history:
{chatHistory}

User: {input}
AI:
`)

const chatChain = RunnableSequence.from([chatPrompt, model, (out) => out.content.trim()])

/**
 * 3ï¸âƒ£ Nodes
 */
async function memoryNodeInput(state: typeof StateAnnotation.State) {
  console.log('ğŸ§  Adding user input to memory...')
  return { chatHistory: [`Human: ${state.input}`] }
}

async function llmNode(state: typeof StateAnnotation.State) {
  console.log('ğŸ’¬ Generating reply...')
  const historyText = (state.chatHistory ?? []).join('\n')
  const response = await chatChain.invoke({
    input: state.input,
    chatHistory: historyText,
  })
  return { response }
}

async function memoryNodeOutput(state: typeof StateAnnotation.State) {
  console.log('ğŸ§  Adding assistant reply to memory...')
  return { chatHistory: [`AI: ${state.response}`] }
}

/**
 * 4ï¸âƒ£ Graph structure
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('memoryInput', memoryNodeInput)
  .addNode('llm', llmNode)
  .addNode('memoryOutput', memoryNodeOutput)
  .addEdge(START, 'memoryInput')
  .addEdge('memoryInput', 'llm')
  .addEdge('llm', 'memoryOutput')
  .addEdge('memoryOutput', END)

const app = workflow.compile()

/**
 * 5ï¸âƒ£ Demo
 */
async function runChatDemo() {
  console.log('\n=== 7.3.1 â€” Graph Memory Basics ===\n')

  const input1 = 'Hi there!'
  const first = await app.invoke({ input: input1 })
  console.log('\nğŸ¤– Response 1:', first.response)

  const input2 = 'Remember my previous message and tell me what I said.'
  const second = await app.invoke({
    input: input2,
    chatHistory: first.chatHistory,
  })
  console.log('\nğŸ¤– Response 2:', second.response)

  console.log('\nğŸ§  Final chat history:\n', second.chatHistory.join('\n'))
}

runChatDemo().catch(console.error)
```

---

# ğŸ’¡ **5. NOTES & TIPS**

### âœ… Memory = State + Reducers

Reducers define how memory accumulates.
They are **pure functions** that merge new updates into the graphâ€™s global state.

---

### âœ… Each Node is Stateless, but Graph is Stateful

Nodes donâ€™t know about memory themselves â€” the graph injects memory via state propagation.

---

### âœ… Use Cases:

- Conversational AI flows
- State-aware assistants (customer context)
- Multi-turn reasoning pipelines
