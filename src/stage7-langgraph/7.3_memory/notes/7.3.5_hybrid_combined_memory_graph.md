# **7.3.5 ‚Äî Hybrid Memory + RAG Reasoning Loop**

### üß† What this graph will do

1. **Store** conversation history incrementally (`chatHistory`)
2. **Summarize** older history when it grows (`summary`)
3. **Retrieve** external knowledge via vector search (`RAG`)
4. **Loop** if needed until a complete answer is formed

This is a **‚ÄúMini Devin-style thinking agent‚Äù** pattern.

---

## **1Ô∏è‚É£ GOAL ‚Äî What You Will Build**

Produce a **context-aware answer** using **all available intelligence layers**:

```
User Input
   ‚Üì
Memory Node ‚Üí (stores message)
   ‚Üì
Summarizer Node ‚Üí (creates short summary)
   ‚Üì
RAG Retriever Node ‚Üí (fetch relevant docs)
   ‚Üì
LLM Answer Node ‚Üí (uses input + summary + retrieved docs)
   ‚Üì
Loop if incomplete
```

‚úîÔ∏è Uses **reducers** for cumulative memory
‚úîÔ∏è Runs multiple **cognition passes**
‚úîÔ∏è Pulls **relevant information dynamically**

---

## **2Ô∏è‚É£ THEORY ‚Äî Mental Model**

### 2.1 Three Memory Layers

| Layer              | Component       | Purpose                 |
| ------------------ | --------------- | ----------------------- |
| Short-Term Memory  | `chatHistory[]` | Immediate turns context |
| Long-Term Memory   | `summary`       | Compressed knowledge    |
| External Knowledge | RAG             | Domain-specific data    |

---

### 2.2 Completion Check

We need a rule to detect whether another reasoning loop is required.

Strategy used here:
‚Üí LLM says `"ANSWER_READY"` or `"NEED_MORE_INFO"`

---

### 2.3 Execution Steps

| Node           | Role                        |
| -------------- | --------------------------- |
| memoryNode     | Store user input            |
| summarizerNode | Updates summary             |
| ragNode        | Retrieve relevant knowledge |
| answerNode     | Produces contextual answer  |
| checkerNode    | Decide continue / stop      |

---

## **3Ô∏è‚É£ CODE ‚Äî Plug-and-Play Hybrid Graph**

> üìÑ File: `graph-hybrid-memory-rag.ts`

> **Note:** This uses a mock RAG for simplicity ‚Äî plug in FAISS / Pinecone easily.

```ts
import { Annotation, StateGraph, START, END } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'

// -------------------------------
// 1Ô∏è‚É£ State Model with Reducers
// -------------------------------

const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  summary: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
  retrievedDocs: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  answer: Annotation<string>(),
  done: Annotation<boolean>({
    reducer: (_, next) => next,
  }),
})

// ----------------------------
// 2Ô∏è‚É£ LLM Setup
// ----------------------------

const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.3,
})

// ----------------------------
// 3Ô∏è‚É£ Prompts & Chains
// ----------------------------

// Format chat history during inference
const formatHistory = (history?: string[]) => (history ?? []).join('\n')

// Summarizer prompt
const summaryPrompt = ChatPromptTemplate.fromTemplate(`
Summarize this conversation into a concise form:

Chat History:
{history}

Existing Summary:
{summary}

Return the improved summary only.
`)

const summaryChain = RunnableSequence.from([summaryPrompt, model, (out) => out.content.trim()])

// Mock RAG function ‚Äî Replace with real vector store later
async function mockRetrieve(query: string) {
  return [`üìÑ Mock doc snippet relevant to: "${query}"`, `üìÑ Additional hint for processing query: "${query}"`]
}

// Answer generation prompt
const answerPrompt = ChatPromptTemplate.fromTemplate(`
You are an advanced AI assistant.

Use:
1. Conversation Summary: {summary}
2. Query: {input}
3. Relevant Docs: {retrieved}

Respond clearly. At end add tag:
- "ANSWER_READY" if final answer
- "NEED_MORE_INFO" if user must clarify
`)

const answerChain = RunnableSequence.from([answerPrompt, model, (out) => out.content.trim()])

// ---------------------------------
// 4Ô∏è‚É£ Node Implementations
// ---------------------------------

async function memoryNode(state: typeof StateAnnotation.State) {
  return { chatHistory: [`Human: ${state.input}`] }
}

async function summarizerNode(state: typeof StateAnnotation.State) {
  const history = formatHistory(state.chatHistory)
  const newSummary = await summaryChain.invoke({
    history,
    summary: state.summary ?? '',
  })
  return { summary: newSummary }
}

async function ragNode(state: typeof StateAnnotation.State) {
  const docs = await mockRetrieve(state.input)
  return { retrievedDocs: docs }
}

async function answerNode(state: typeof StateAnnotation.State) {
  const answer = await answerChain.invoke({
    summary: state.summary ?? '',
    input: state.input,
    retrieved: (state.retrievedDocs ?? []).join('\n'),
  })
  return { answer }
}

async function checkerNode(state: typeof StateAnnotation.State) {
  if (state.answer?.includes('ANSWER_READY')) {
    return { done: true }
  }
  return { done: false }
}

// ---------------------------------
// 5Ô∏è‚É£ Graph Definition
// ---------------------------------

const workflow = new StateGraph(StateAnnotation)
  .addNode('memory', memoryNode)
  .addNode('summarizer', summarizerNode)
  .addNode('retriever', ragNode)
  .addNode('answerNode', answerNode)
  .addNode('checker', checkerNode)
  .addEdge(START, 'memory')
  .addEdge('memory', 'summarizer')
  .addEdge('summarizer', 'retriever')
  .addEdge('retriever', 'answerNode')
  .addEdge('answerNode', 'checker')
  .addConditionalEdges(
    'checker',
    (state) => (state.done ? 'END' : 'memory'), // loop until done
    {
      END,
      memory: 'memory',
    },
  )

const app = workflow.compile()

// ---------------------------------
// 6Ô∏è‚É£ Demo Runner
// ---------------------------------

async function runDemo() {
  const input1 = 'Teach me about defensive driving tips.'
  const res1 = await app.invoke({ input: input1 })
  console.log('\nAI:', res1.answer)

  const input2 = 'What should I do while driving at night?'
  const res2 = await app.invoke({
    input: input2,
    chatHistory: res1.chatHistory,
    summary: res1.summary,
  })

  console.log('\nAI:', res2.answer)
  console.log('\nüß† Final Memory Summary:', res2.summary)
}

runDemo().catch(console.error)
```

---

## **4Ô∏è‚É£ NOTES & BEST PRACTICES**

| Rule                      | Guidance                                            |
| ------------------------- | --------------------------------------------------- |
| Reducers = Memory         | Always use reducers for persistent fields           |
| RAG ‚â† Memory              | RAG is external knowledge, memory is conversational |
| Summary updates gradually | Do not rewrite full summary every time              |
| Loop until certainty      | Use tags or schema result for loop exit             |
| Swap RAG easily           | FAISS, Chroma, Pinecone, Weaviate                   |

---

### üéØ Improvements you can try

| Idea                           | Example                     |
| ------------------------------ | --------------------------- |
| Add hallucination guard        | Require citation count      |
| Use embedding-based summarizer | Replace LLM summarizer      |
| Add planner stage              | Let agent decide next node  |
| Human feedback                 | HITL re-route to fix memory |

---

### üî• You‚Äôve now built:

‚úîÔ∏è Multi-layer memory
‚úîÔ∏è Hybrid context architecture
‚úîÔ∏è Self-refining iterative graph
‚úîÔ∏è Foundation for **Devin-style agents**
