# ‚úÖ **7.3.3 ‚Äî RAG Node (Retrieval-Augmented Graph)**

---

## ü•Ö **1. GOAL**

You will enhance your LangGraph workflow by adding a **Retrieval-Augmented Generation (RAG) node** that:

- ‚úÖ Retrieves relevant chunks from a vector store
- ‚úÖ Injects them into the prompt dynamically
- ‚úÖ Combines retrieval context + memory context
- ‚úÖ Responds using both long-term memory and external knowledge

---

### üí° **End Output**

A graph that can answer:

> ‚ÄúWhat are the key points about RAG from our stored knowledge base?‚Äù
> by fetching from an embedded document source **inside** the graph‚Äôs node pipeline.

---

## üß† **2. THEORY**

---

### üìò 2.1 What Is RAG?

RAG = **Retrieval-Augmented Generation**

A hybrid of:

1. **Retriever** ‚Üí Fetch relevant documents
2. **Generator (LLM)** ‚Üí Answer using those docs + question

---

### üìò 2.2 Why RAG in LangGraph?

LangGraph gives us a way to treat **retrieval as a node**, so it‚Äôs deterministic, observable, and can connect conditionally (e.g., "retrieve only if needed").

```
User Input
   ‚Üì
Classifier Node  ‚Üí (RAG needed?) ‚Üí Yes ‚Üí Retrieval Node
                                     ‚Üì
                                LLM (Generate answer)
```

---

### üìò 2.3 RAG Node Structure in Graph

We‚Äôll add a `retrievalNode` before the `llmNode`:

```
Input
  ‚Üì
MemoryInput ‚Üí RetrievalNode ‚Üí LLMNode ‚Üí MemoryOutput
                                      ‚Üì
                                     END
```

Each run will:

- Query a retriever (FAISS or MemoryVectorStore)
- Attach results as `retrievedDocs`
- Use them in the final generation

---

### üìò 2.4 Minimal Retriever Setup

We'll use:

- `OpenAIEmbeddings` (or HuggingFace)
- `MemoryVectorStore` (in-memory vector DB)
- Simple `Retriever` instance from LangChain

---

## ‚öôÔ∏è **3. CODE SNIPPETS (Step-by-Step)**

---

### ‚úÖ 3.1 Define State

```ts
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  summary: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  retrievedDocs: Annotation<string[]>({
    reducer: (curr, next) => next ?? curr,
  }),
  response: Annotation<string>(),
})
```

---

### ‚úÖ 3.2 Prepare Vector Store (Knowledge Base)

```ts
import { OpenAIEmbeddings } from '@langchain/openai'
import { MemoryVectorStore } from 'langchain/vectorstores/memory'

// Dummy knowledge base
const documents = [
  'RAG stands for Retrieval-Augmented Generation.',
  'LangGraph supports building graph-based LLM workflows.',
  'Memory nodes help maintain conversation context.',
  'Vector stores like FAISS or Chroma are used for retrieval.',
]

const embeddings = new OpenAIEmbeddings({ model: 'text-embedding-3-small' })
const vectorStore = await MemoryVectorStore.fromTexts(documents, {}, embeddings)
const retriever = vectorStore.asRetriever(2)
```

---

### ‚úÖ 3.3 Retrieval Node

```ts
async function retrievalNode(state) {
  console.log('üîç Retrieving relevant docs...')
  const results = await retriever.invoke(state.input)
  const retrievedDocs = results.map((d) => d.pageContent)
  console.log('üìÑ Retrieved Docs:', retrievedDocs)
  return { retrievedDocs }
}
```

---

### ‚úÖ 3.4 LLM Node (RAG + Memory aware)

```ts
const ragPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.

Summary Memory:
{summary}

Recent Chat History:
{chatHistory}

Retrieved Knowledge:
{retrievedDocs}

User question: {input}

Answer concisely, using both memory and retrieved information.
`)

const ragChain = RunnableSequence.from([ragPrompt, model, (out) => out.content.trim()])

async function llmNode(state) {
  console.log('üí¨ Generating contextual response...')
  const response = await ragChain.invoke({
    input: state.input,
    chatHistory: (state.chatHistory ?? []).join('\n'),
    summary: state.summary || 'None yet.',
    retrievedDocs: (state.retrievedDocs ?? []).join('\n'),
  })
  return { response }
}
```

---

### ‚úÖ 3.5 Memory Nodes (same as before)

```ts
async function memoryInputNode(state) {
  return { chatHistory: [`Human: ${state.input}`] }
}

async function memoryOutputNode(state) {
  return { chatHistory: [`AI: ${state.response}`] }
}
```

---

# üíª **4. FINAL FILE (Plug-and-Play Code)**

üìÑ `graph-rag-node.ts`

```ts
import { Annotation, StateGraph, START, END } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { OpenAIEmbeddings } from '@langchain/openai'
import { MemoryVectorStore } from 'langchain/vectorstores/memory'
import 'dotenv/config'

/**
 * 1Ô∏è‚É£ State Definition
 */
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  summary: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  retrievedDocs: Annotation<string[]>({
    reducer: (curr, next) => next ?? curr,
  }),
  response: Annotation<string>(),
})

/**
 * 2Ô∏è‚É£ Models & Stores
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.4,
})

const documents = [
  'RAG stands for Retrieval-Augmented Generation.',
  'LangGraph allows building graph-based workflows for LLMs.',
  'Memory nodes maintain stateful conversation context.',
  'Vector stores such as FAISS or Chroma store text embeddings for retrieval.',
]

const embeddings = new OpenAIEmbeddings({ model: 'text-embedding-3-small' })
const vectorStore = await MemoryVectorStore.fromTexts(documents, {}, embeddings)
const retriever = vectorStore.asRetriever(2)

/**
 * 3Ô∏è‚É£ Retrieval Node
 */
async function retrievalNode(state: typeof StateAnnotation.State) {
  console.log('üîç Retrieving relevant docs...')
  const results = await retriever.invoke(state.input)
  const retrievedDocs = results.map((d) => d.pageContent)
  console.log('üìÑ Retrieved Docs:', retrievedDocs)
  return { retrievedDocs }
}

/**
 * 4Ô∏è‚É£ RAG Chain
 */
const ragPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.
Use both retrieved knowledge and conversation memory to answer accurately.

Summary Memory:
{summary}

Recent Chat:
{chatHistory}

Retrieved Knowledge:
{retrievedDocs}

User Question:
{input}
`)

const ragChain = RunnableSequence.from([ragPrompt, model, (out) => out.content.trim()])

async function llmNode(state: typeof StateAnnotation.State) {
  console.log('üí¨ Generating contextual answer...')
  const response = await ragChain.invoke({
    input: state.input,
    summary: state.summary || 'None yet.',
    chatHistory: (state.chatHistory ?? []).join('\n'),
    retrievedDocs: (state.retrievedDocs ?? []).join('\n'),
  })
  return { response }
}

/**
 * 5Ô∏è‚É£ Memory Nodes
 */
async function memoryInputNode(state) {
  return { chatHistory: [`Human: ${state.input}`] }
}

async function memoryOutputNode(state) {
  return { chatHistory: [`AI: ${state.response}`] }
}

/**
 * 6Ô∏è‚É£ Graph Construction
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('memoryInput', memoryInputNode)
  .addNode('retrievalNode', retrievalNode)
  .addNode('llmNode', llmNode)
  .addNode('memoryOutput', memoryOutputNode)
  .addEdge(START, 'memoryInput')
  .addEdge('memoryInput', 'retrievalNode')
  .addEdge('retrievalNode', 'llmNode')
  .addEdge('llmNode', 'memoryOutput')
  .addEdge('memoryOutput', END)

const app = workflow.compile()

/**
 * 7Ô∏è‚É£ Demo Run
 */
async function main() {
  console.log('\n=== 7.3.3 ‚Äî Retrieval-Augmented Graph ===\n')

  let state = { summary: '', chatHistory: [] }

  const input = 'What does RAG mean, and how does LangGraph relate to it?'
  console.log('\nüë§ User:', input)
  const output = await app.invoke({ ...state, input })

  console.log('\nü§ñ AI Response:\n', output.response)
}

main().catch(console.error)
```

---

# üí° **5. NOTES & BEST PRACTICES**

| Concept              | Explanation                                                          |
| -------------------- | -------------------------------------------------------------------- |
| **Retriever Node**   | A deterministic, stateless node that injects external data           |
| **Embedding Model**  | Use small models for local retrieval (like `text-embedding-3-small`) |
| **Reducer Behavior** | Retrieval results should **replace**, not append                     |
| **Prompt Context**   | Combine `{summary}`, `{chatHistory}`, and `{retrievedDocs}`          |
| **Evaluation Tip**   | Ask multiple related questions to see retrieval persistency          |
| **Scaling Up**       | Replace MemoryVectorStore ‚Üí FAISS / Chroma / Pinecone                |

---

# ‚úÖ **You Now Have:**

‚úÖ A memory-aware graph
‚úÖ Retrieval capability
‚úÖ Context fusion between short-term, long-term, and external knowledge
