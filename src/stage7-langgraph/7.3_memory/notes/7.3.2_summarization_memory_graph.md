# ‚úÖ **7.3.2 ‚Äî Summary Memory (Compress Old Chat Messages)**

---

## ü•Ö **1. GOAL**

We‚Äôll extend the previous ‚ÄúGraph Memory Basics‚Äù example to create a **summary memory graph** that:

- ‚úÖ Keeps a rolling `chatHistory` array (like before)
- ‚úÖ Periodically summarizes old messages into a `summary` field
- ‚úÖ Retains both `summary` + `recent messages`
- ‚úÖ Uses that compressed state in future LLM calls

**Outcome:**

> An efficient ‚Äúpersistent summary memory‚Äù graph that mimics _ChatGPT‚Äôs long-term conversation compression logic_.

---

## üß† **2. THEORY (Concepts You Need)**

### üìò 2.1 Why Summary Memory?

LLMs have **context length limits** (e.g., 128k tokens for GPT-4o).
To maintain _long conversations_:

- We **summarize** older messages periodically.
- We **merge** that summary into state as compressed context.

Instead of this:

```
[Human: Hi] [AI: Hello!] [Human: I like cooking] [AI: Oh great!]
...
```

We store:

```
Summary: "User and AI discussed hobbies like cooking."
Recent:  [Human: Let's talk about recipes...]
```

---

### üìò 2.2 How It Works (LangGraph Style)

LangGraph state can contain both:

- `chatHistory`: recent short-term memory
- `summary`: long-term compressed memory

A **Summary Node** runs:

1. Every N turns (or when chatHistory > limit)
2. Generates a new summary from existing `summary + chatHistory`
3. Resets or truncates `chatHistory`

---

### üìò 2.3 The Graph Flow

```
Input
  ‚Üì
MemoryNode (append user input)
  ‚Üì
LLMNode (generate response using summary + chatHistory)
  ‚Üì
MemoryNode (append LLM reply)
  ‚Üì
SummaryNode (if chatHistory too long ‚Üí compress)
  ‚Üì
END
```

---

## ‚öôÔ∏è **3. CODE SNIPPETS (Step-by-Step)**

---

### ‚úÖ 3.1 State Model

```ts
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  summary: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
  response: Annotation<string>(),
})
```

---

### ‚úÖ 3.2 Summarizer Model & Prompt

```ts
const summarizerPrompt = ChatPromptTemplate.fromTemplate(`
Summarize the following chat into a short paragraph that captures key facts and context.

Previous summary:
{summary}

Recent conversation:
{chatHistory}

Return only the new updated summary.
`)
```

---

### ‚úÖ 3.3 Summary Chain

```ts
const summarizeChain = RunnableSequence.from([summarizerPrompt, model, (out) => out.content.trim()])
```

---

### ‚úÖ 3.4 Summary Node Logic

```ts
async function summaryNode(state) {
  const history = state.chatHistory ?? []
  if (history.length < 4) {
    // Skip summarization if conversation is short
    return {}
  }

  console.log('üßæ Summarizing conversation history...')

  const newSummary = await summarizeChain.invoke({
    summary: state.summary || 'None yet.',
    chatHistory: history.join('\n'),
  })

  // Reset short-term history (simulate sliding window)
  return {
    summary: newSummary,
    chatHistory: [], // reset recent messages
  }
}
```

---

### ‚úÖ 3.5 LLM Node (uses both memory types)

```ts
const chatPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.
Use the provided summary (long-term memory) and recent chat history.

Summary memory:
{summary}

Recent chat history:
{chatHistory}

User: {input}
AI:
`)
```

---

# üíª **4. FINAL FILE (Plug-and-Play Code)**

üìÑ `graph-summary-memory.ts`

```ts
import { Annotation, StateGraph, START, END } from '@langchain/langgraph'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { ChatOpenAI } from '@langchain/openai'
import { RunnableSequence } from '@langchain/core/runnables'
import 'dotenv/config'

/**
 * 1Ô∏è‚É£ State model
 */
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  chatHistory: Annotation<string[]>({
    reducer: (curr, next) => [...(curr ?? []), ...(next ?? [])],
  }),
  summary: Annotation<string>({
    reducer: (curr, next) => next ?? curr,
  }),
  response: Annotation<string>(),
})

/**
 * 2Ô∏è‚É£ Models
 */
const model = new ChatOpenAI({
  modelName: 'gpt-4o-mini',
  temperature: 0.5,
})

/**
 * 3Ô∏è‚É£ Chat chain
 */
const chatPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.
Use the provided summary (long-term memory) and recent chat history.

Summary memory:
{summary}

Recent chat history:
{chatHistory}

User: {input}
AI:
`)

const chatChain = RunnableSequence.from([chatPrompt, model, (out) => out.content.trim()])

/**
 * 4Ô∏è‚É£ Summarizer chain
 */
const summarizerPrompt = ChatPromptTemplate.fromTemplate(`
Summarize the following chat into a concise paragraph that preserves facts and tone.

Previous summary:
{summary}

Recent conversation:
{chatHistory}

Return only the updated summary.
`)

const summarizeChain = RunnableSequence.from([summarizerPrompt, model, (out) => out.content.trim()])

/**
 * 5Ô∏è‚É£ Nodes
 */
async function memoryInputNode(state: typeof StateAnnotation.State) {
  console.log('üß† Adding user input to chat history...')
  return { chatHistory: [`Human: ${state.input}`] }
}

async function llmNode(state: typeof StateAnnotation.State) {
  console.log('üí¨ Generating reply using summary + history...')
  const historyText = (state.chatHistory ?? []).join('\n')
  const response = await chatChain.invoke({
    input: state.input,
    chatHistory: historyText,
    summary: state.summary || 'None yet.',
  })
  return { response }
}

async function memoryOutputNode(state: typeof StateAnnotation.State) {
  console.log('üß† Adding AI reply to chat history...')
  return { chatHistory: [`AI: ${state.response}`] }
}

async function summaryNode(state: typeof StateAnnotation.State) {
  const history = state.chatHistory ?? []
  if (history.length < 4) {
    return {} // skip summarization if few turns
  }

  console.log('üßæ Summarizing long chat history...')

  const newSummary = await summarizeChain.invoke({
    summary: state.summary || 'None yet.',
    chatHistory: history.join('\n'),
  })

  console.log('üìò Summary updated:\n', newSummary)

  // reset short-term buffer
  return {
    summary: newSummary,
    chatHistory: [],
  }
}

/**
 * 6Ô∏è‚É£ Graph
 */
const workflow = new StateGraph(StateAnnotation)
  .addNode('memoryInput', memoryInputNode)
  .addNode('llm', llmNode)
  .addNode('memoryOutput', memoryOutputNode)
  .addNode('summary', summaryNode)
  .addEdge(START, 'memoryInput')
  .addEdge('memoryInput', 'llm')
  .addEdge('llm', 'memoryOutput')
  .addEdge('memoryOutput', 'summary')
  .addEdge('summary', END)

const app = workflow.compile()

/**
 * 7Ô∏è‚É£ Demo
 */
async function main() {
  console.log('\n=== 7.3.2 ‚Äî Summary Memory Graph ===\n')

  let state = { summary: '', chatHistory: [] }

  const turns = [
    'Hey there! I work as a chef.',
    'I love making Italian food, especially pasta.',
    'Can you suggest a dessert that pairs well with pasta?',
    'Also, remind me what I told you about my job earlier.',
  ]

  for (const input of turns) {
    console.log('\nüë§ User:', input)
    state = await app.invoke({ ...state, input })
    console.log('ü§ñ AI:', state.response)
  }

  console.log('\nüß† Final Summary:\n', state.summary)
}

main().catch(console.error)
```

---

## üí° **5. NOTES & BEST PRACTICES**

| Concept                | Explanation                                                                                 |
| ---------------------- | ------------------------------------------------------------------------------------------- |
| **State as Memory**    | Summary = long-term memory, ChatHistory = short-term buffer                                 |
| **Reducer Importance** | Summary uses ‚Äúreplace‚Äù, ChatHistory uses ‚Äúappend‚Äù                                           |
| **When to Summarize**  | Based on message count or token limit                                                       |
| **How to Merge**       | Use both `{summary}` and `{chatHistory}` in the prompt                                      |
| **Result**             | Efficient, cost-saving, contextual memory                                                   |
| **Scaling**            | Replace `summarizerNode` with your own summarization LLM or a fine-tuned distillation model |

---

### üß© What You Just Built

‚úÖ Rolling conversational memory
‚úÖ Automatic summarization
‚úÖ Graph-level memory compression
‚úÖ Perfect base for persistent memory or RAG graphs
