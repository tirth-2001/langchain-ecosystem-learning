# 7.4.4 â€” HITL + Tool (Approve tool calls before execution)

---

## 1) Goal

Build a LangGraph workflow where:

- The graph **decides** whether a user query requires a tool (e.g., calculator).
- If a tool is required, the graph **pauses** and asks a human to **approve or deny** the tool call.
- On human approval, the graph **executes the tool** and returns the result.
- On denial, the graph **skips tool** and the LLM answers directly.
- Use `interrupt()` + `Command.resume` + a persistent checkpointer so pause/resume is reliable across restarts.

Flow:

```
User Input
   â†“
classifierNode  â”€â”€"llm"â”€â”€> llmAnswerNode â”€â”€> finalize
      â””â”€"tool"â”€> approvalNode (interrupt)
                    â”œâ”€approvedâ”€â”€> toolExecuteNode â†’ toolAnswerNode
                    â””â”€deniedâ”€â”€â”€â”€> llmAnswerNode
```

---

## 2) Theory & Concepts

### Why HITL before tool execution?

- Some tool calls may have cost, privacy, or security implications.
- Human approval provides a safety & compliance gate.
- Approval is helpful when tool actions are destructive (DB writes), sensitive, or costly.

### Key LangGraph primitives used

- `interrupt({ prompt, data })` inside a node to pause and present a prompt + context to human.
- Resume using `new Command({ resume: <value> })` when calling `app.invoke()` the second time.
- `MemorySaver` or other `checkpointer` required for interrupt/resume to work â€” it stores execution state so the graph picks up exactly where it paused.

### Important behaviour

- When `interrupt()` is called, the graph run pauses and a checkpoint is saved.
- On resume, the node that called `interrupt()` re-runs; `interrupt()` returns the `resume` payload you passed with the `Command`.
- Use reducers to persist flags like `approved` across runs.

---

## 3) Code snippets â€” step-by-step

Below are the key pieces. After these snippets you'll get the full file.

### 3.1 State model (annotations & reducers)

```ts
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  route: Annotation<string>(), // "tool" | "llm"
  toolArgs: Annotation<string>({ reducer: (_, u) => u ?? _ }),
  approved: Annotation<boolean | null>({ reducer: (c, u) => (u === undefined ? c : u) }),
  toolResult: Annotation<string>({ reducer: (_, u) => u ?? _ }),
  final: Annotation<string>({ reducer: (_, u) => u ?? _ }),
})
```

### 3.2 Classifier node (decide tool vs LLM)

```ts
const classifyPrompt = ChatPromptTemplate.fromTemplate(`
Decide whether the user input requires a calculator.

User: "{input}"

Reply with exactly one token:
- "tool"  (if it needs calculation)
- "llm"   (otherwise)
`)

const classifyChain = RunnableSequence.from([classifyPrompt, model, (o) => o.content.trim().toLowerCase()])

async function classifierNode(state) {
  const route = await classifyChain.invoke({ input: state.input })
  console.log('Classifier ->', route)
  // For tool route we set toolArgs to the expression (for simplicity)
  if (route === 'tool') {
    return { route, toolArgs: state.input }
  }
  return { route }
}
```

### 3.3 Approval node (HITL pause)

```ts
async function approvalNode(state) {
  console.log('Approval node reached. Prompt human with toolArgs:', state.toolArgs)

  // Pause execution and ask human to approve or deny.
  // The human UI should present: prompt + state.data (draft/toolArgs)
  // When resuming, pass Command({ resume: { approved: true/false } })
  const resumePayload = interrupt({
    prompt: `Approve the tool call for: "${state.toolArgs}"? Reply with { approved: true } or { approved: false }`,
    data: { toolArgs: state.toolArgs },
  })

  // resumePayload is the object passed in Command.resume when resuming
  // normalize it to boolean
  const approved = !!resumePayload?.approved
  console.log('Resumed approval value:', approved)
  return { approved }
}
```

### 3.4 Tool execution node (internal calculator)

```ts
function calculatorTool(expression: string): number | null {
  try {
    const safeExpr = expression.replace(/[^0-9+\-*/().\s]/g, '')
    // eslint-disable-next-line no-eval
    const val = eval(safeExpr)
    if (typeof val === 'number' && Number.isFinite(val)) return val
    return null
  } catch {
    return null
  }
}

async function toolExecuteNode(state) {
  console.log('Executing calculator tool for:', state.toolArgs)
  const result = calculatorTool(state.toolArgs ?? '')
  if (result === null) {
    return { toolResult: 'Error: tool failed to compute.' }
  }
  return { toolResult: result.toString() }
}
```

### 3.5 LLM answer node (fallback or post-deny)

```ts
const fallbackPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.
User asked: {input}

If a tool result is available, use it: {toolResult}
Otherwise answer conversationally.
`)

const fallbackChain = RunnableSequence.from([fallbackPrompt, model, (o) => o.content.trim()])

async function llmAnswerNode(state) {
  const answer = await fallbackChain.invoke({
    input: state.input,
    toolResult: state.toolResult ?? 'No tool result',
  })
  return { final: answer }
}
```

### 3.6 Tool answer formatter (LLM formats final response after tool run)

```ts
const toolAnswerPrompt = ChatPromptTemplate.fromTemplate(`
User asked: {input}
Tool output: {toolResult}

Write a brief, friendly final answer combining both.
`)
const toolAnswerChain = RunnableSequence.from([toolAnswerPrompt, model, (o) => o.content.trim()])

async function toolAnswerNode(state) {
  const final = await toolAnswerChain.invoke({ input: state.input, toolResult: state.toolResult ?? '' })
  return { final }
}
```

### 3.7 Graph wiring: conditional edges

- `START -> classifierNode`
- `classifierNode` conditional: route -> 'tool' => `approvalNode`, 'llm' => `llmAnswerNode`
- `approvalNode` conditional on `approved` -> `toolExecuteNode` or `llmAnswerNode`
- if `toolExecuteNode` -> `toolAnswerNode` -> END
- `llmAnswerNode` -> END

Use `addConditionalEdges` for classifier and approval nodes.

---

## 4) Final plug-and-play file

Save as `graph-hitl-tool.ts`. Run with `ts-node` (Node >= 18 or with node-fetch installed). It demonstrates both approve and deny flows â€” first run pauses; you resume programmatically via `Command` to simulate a human.

```ts
/**
 * graph-hitl-tool.ts
 *
 * 7.4.4 â€” HITL + Tool Node Demo
 */
import 'dotenv/config'
import fetch from 'node-fetch' // optional, not used here but typical in web-tools
import { Annotation, StateGraph, START, END, interrupt, Command, MemorySaver } from '@langchain/langgraph'
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'

// ======================
// 1) State definition
// ======================
const StateAnnotation = Annotation.Root({
  input: Annotation<string>(),
  route: Annotation<string>(),
  toolArgs: Annotation<string>({ reducer: (_, u) => u ?? _ }),
  approved: Annotation<boolean | null>({ reducer: (c, u) => (u === undefined ? c : u) }),
  toolResult: Annotation<string>({ reducer: (_, u) => u ?? _ }),
  final: Annotation<string>({ reducer: (_, u) => u ?? _ }),
})

// ======================
// 2) Model (LLM)
// ======================
const model = new ChatOpenAI({ modelName: 'gpt-4o-mini', temperature: 0 })

// ======================
// 3) Nodes
// ======================

// 3.1 Classifier â€” decide tool vs LLM
const classifyPrompt = ChatPromptTemplate.fromTemplate(`
Classify whether the user input requires a calculator ("tool") or can be answered by LLM ("llm").
Return a single word: tool or llm.

User: "{input}"
`)
const classifyChain = RunnableSequence.from([classifyPrompt, model, (o) => o.content.trim().toLowerCase()])

async function classifierNode(state: typeof StateAnnotation.State) {
  const route = await classifyChain.invoke({ input: state.input })
  console.log('Classifier ->', route)
  if (route === 'tool') {
    // set tool arguments (for demo we simply pass the input expression)
    return { route, toolArgs: state.input }
  }
  return { route }
}

// 3.2 Approval Node â€” pause & request human approval
async function approvalNode(state: typeof StateAnnotation.State) {
  console.log('Approval node reached. Tool args:', state.toolArgs)

  // Pause: show prompt + draft/toolArgs. UI should capture resume payload.
  const resumePayload = interrupt({
    prompt: `Approve or deny execution of the tool for expression: "${state.toolArgs}". Return { approved: true } or { approved: false }`,
    data: { toolArgs: state.toolArgs },
  })

  // On resume, Command.resume payload will be returned here.
  const approved = !!resumePayload?.approved
  console.log('Approval resumed with:', approved)
  return { approved }
}

// 3.3 Tool execution (calculator)
function calculatorTool(expression: string): number | null {
  try {
    const safeExpr = expression.replace(/[^0-9+\-*/().\s]/g, '')
    // eslint-disable-next-line no-eval
    const val = eval(safeExpr)
    if (typeof val === 'number' && Number.isFinite(val)) return val
    return null
  } catch {
    return null
  }
}

async function toolExecuteNode(state: typeof StateAnnotation.State) {
  console.log('Executing tool with args:', state.toolArgs)
  const out = calculatorTool(state.toolArgs ?? '')
  if (out === null) {
    return { toolResult: 'Error: tool execution failed' }
  }
  return { toolResult: out.toString() }
}

// 3.4 Tool answer formatter (LLM)
const toolAnswerPrompt = ChatPromptTemplate.fromTemplate(`
User asked: {input}
Tool output: {toolResult}

Write a brief, friendly final answer combining both.
`)
const toolAnswerChain = RunnableSequence.from([toolAnswerPrompt, model, (o) => o.content.trim()])

async function toolAnswerNode(state: typeof StateAnnotation.State) {
  const final = await toolAnswerChain.invoke({ input: state.input, toolResult: state.toolResult ?? '' })
  return { final }
}

// 3.5 LLM fallback/deny node
const fallbackPrompt = ChatPromptTemplate.fromTemplate(`
You are a helpful assistant.
User: {input}
If no tool result is available, answer conversationally.
`)
const fallbackChain = RunnableSequence.from([fallbackPrompt, model, (o) => o.content.trim()])

async function llmAnswerNode(state: typeof StateAnnotation.State) {
  const final = await fallbackChain.invoke({ input: state.input })
  return { final }
}

// ======================
// 4) Graph wiring with conditional edges
// ======================

const workflow = new StateGraph(StateAnnotation)
  .addNode('classifier', classifierNode)
  .addNode('approval', approvalNode)
  .addNode('toolExec', toolExecuteNode)
  .addNode('toolAnswer', toolAnswerNode)
  .addNode('llmAnswer', llmAnswerNode)
  .addEdge(START, 'classifier')
  // classifier decides: 'tool' -> approval, 'llm' -> llmAnswer
  .addConditionalEdges('classifier', (state) => state.route, {
    tool: 'approval',
    llm: 'llmAnswer',
  })
  // approval routes to toolExec if approved, else llmAnswer
  .addConditionalEdges('approval', (state) => (state.approved ? 'approved' : 'denied'), {
    approved: 'toolExec',
    denied: 'llmAnswer',
  })
  .addEdge('toolExec', 'toolAnswer')
  .addEdge('toolAnswer', END)
  .addEdge('llmAnswer', END)

// Checkpointer (MemorySaver for demo). Use Redis/Mongo saver for production.
const checkpointer = new MemorySaver()

const app = workflow.compile({
  checkpointer, // required for interrupts to resume properly
})

// ======================
// 5) Demo runner â€” simulate approval / denial with Command.resume
// ======================
async function runDemoApproveThenDeny() {
  const threadId = 'hitl-demo-thread-1'
  const config = { configurable: { thread_id: threadId } }

  console.log('\n=== 7.4.4 â€” HITL + Tool Demo (APPROVE flow) ===\n')

  // STEP 1: initial invoke â€” will pause at approvalNode
  console.log('STEP 1: Invoke and pause for approval...')
  const paused = await app.invoke({ input: 'Compute 12 * (8 + 2)' }, config)

  console.log('\nGraph paused. State snapshot: ', {
    input: paused.input,
    route: paused.route,
    toolArgs: paused.toolArgs,
    approved: paused.approved,
  })

  // Simulate human approval via Command.resume
  console.log('\nSTEP 2: Resume with approval (Command.resume = { approved: true })')
  const resumed = await app.invoke(new Command({ resume: { approved: true } }), config)

  console.log('\nFINAL OUTPUT (after approval):')
  console.log('toolResult:', resumed.toolResult)
  console.log('final:', resumed.final)

  // --- Now demo deny path on a different thread
  const threadId2 = 'hitl-demo-thread-2'
  const cfg2 = { configurable: { thread_id: threadId2 } }

  console.log('\n\n=== Now DEMO DENY flow ===\n')
  console.log('STEP 1: Invoke and pause for approval (deny path)...')
  const paused2 = await app.invoke({ input: 'Compute 5 * 5' }, cfg2)

  console.log('\nGraph paused (deny path). Snapshot:', {
    input: paused2.input,
    route: paused2.route,
    toolArgs: paused2.toolArgs,
    approved: paused2.approved,
  })

  console.log('\nSTEP 2: Resume with denial (Command.resume = { approved: false })')
  const resumed2 = await app.invoke(new Command({ resume: { approved: false } }), cfg2)

  console.log('\nFINAL OUTPUT (after denial):')
  console.log('toolResult:', resumed2.toolResult) // likely undefined
  console.log('final:', resumed2.final)
}

runDemoApproveThenDeny().catch((e) => {
  console.error('Demo error:', e)
  process.exit(1)
})
```

---

## 5) Notes, Tips & Next Steps

### âœ… Behavior checklist

- `approvalNode` uses `interrupt()` â€” graph pauses and is checkpointed.
- On resume, call `app.invoke(new Command({ resume: { approved: true/false } }), config)` to continue.
- The resumed value from `Command.resume` is available inside `approvalNode` as the return of `interrupt()`.

### ðŸ”’ Safety & production notes

- Never pass raw user input to unsafe eval in production. Use `mathjs` or `expr-eval` for arithmetic.
- For sensitive tool calls (DB writes, payment ops), record **who** approved the action and why (store approver id + timestamp).
- Add an approval UI that displays the `prompt` and `data` you supplied to `interrupt()` and allows an authorized user to resume with approval payload.

### ðŸ§° Useful improvements

- Add an explicit `approvalReason` field in resume payload for traceability.
- Add timeouts for approvals: if no resume within X minutes, auto-deny or queue notification.
- Add audit logs to checkpointer records to trace who resumed the thread.
- For concurrent users, use a robust checkpointer (Redis/Mongo) not MemorySaver.

### âœ… Testing tips

- Use separate thread IDs to test different flows.
- Simulate human resume with `Command({ resume: { approved: true } })` as shown.
- Inspect checkpointer storage to debug paused runs.
