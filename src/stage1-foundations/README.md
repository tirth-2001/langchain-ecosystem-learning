# Stage 1 â€“ Foundations & Core Building Blocks

## ğŸ¯ Learning Objectives

- Understand why **LangChain exists** (abstraction & orchestration layer over raw LLM APIs).
- Get familiar with LangChainâ€™s **core building blocks**:
  - `LLM` â€“ large language models.
  - `PromptTemplate` â€“ parameterized prompts.
  - `Chain` â€“ combining LLMs + prompts into workflows.
  - `Agent` â€“ decision-making loop.
  - `Tool` â€“ external functions/APIs the LLM can use.
  - `Memory` â€“ ways to persist conversational state.
- Explore the **LangChain ecosystem**:
  - **LangChain** (framework).
  - **LangGraph** (workflow orchestration).
  - **LangSmith** (debugging, monitoring).
- Compare **Raw OpenAI API vs LangChain abstraction**.
- Track **token usage & execution latency**.

---

## ğŸ§© Core Concepts Covered

### 1. LLMs

- Large Language Models (LLMs) are prediction machines trained on vast text.
- LangChain provides wrappers for many providers (OpenAI, Anthropic, HuggingFace).
- You can swap models without rewriting pipeline logic.

### 2. PromptTemplate

- Allows variable substitution in prompts.
- Encourages reusable, parameterized prompts.
- Helps structure inputs cleanly instead of messy string concatenation.

### 3. Chains

- Chains connect inputs â†’ prompt â†’ LLM â†’ output.
- `LLMChain` = simplest form (prompt + LLM).
- More advanced chains in later stages (Sequential, Router, etc.).

### 4. Ecosystem Preview

- **LangChain Core** â€“ building blocks.
- **LangGraph** â€“ flow-based orchestration for multi-step tasks.
- **LangSmith** â€“ observability (trace steps, costs, tokens).

---

## ğŸ› ï¸ Micro-Project: Hello LangChain

1. **Raw OpenAI API Call**
   - Direct API call using `openai` package.
   - Logs response, tokens, and latency.
2. **LangChain Equivalent**
   - Uses `ChatOpenAI` + `PromptTemplate` + `LLMChain`.
   - Adds `CallbackManager` to capture tokens.
   - Compare with raw API: slightly higher tokens/latency, but better structure and observability.

---

## ğŸ“Š Insights from Results

- **Latency**: Raw API is faster; LangChain adds overhead for orchestration, logging, and structure.
- **Tokens**: LangChain often uses slightly more tokens (adds system messages, template formatting).
- **Trade-off**:
  - Raw API â†’ fastest, minimal abstraction.
  - LangChain â†’ slightly slower but powerful for pipelines, agents, memory, tools.

---

## âœ… Learning Outcomes

By the end of Stage 1 you can:

- Call OpenAI directly and via LangChain.
- Use `PromptTemplate` and `LLMChain`.
- Log tokens and latency with LangChainâ€™s callback system.
- Understand the **why** behind LangChain abstractions.

---

## ğŸ“š References

- [LangChain JS Docs](https://js.langchain.com/docs/)
- [LangChain Core: LLMs](https://js.langchain.com/docs/modules/model_io/llms)
- [OpenAI Chat Models](https://js.langchain.com/docs/integrations/chat/openai)
