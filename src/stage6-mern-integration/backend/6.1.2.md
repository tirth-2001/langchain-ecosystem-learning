## **6.1.2 – LangChain Backend Setup**

### 🧩 **Objective**

Set up LangChain inside your Node/Express backend to make the **first model-driven API route** — i.e., the foundation of the AI Task Hub backend.

This stage bridges your backend with an LLM provider (like OpenAI or Anthropic) and establishes a clean, reusable configuration pattern for future agents, chains, and tools.

---

## **📘 Theory**

Let’s understand the structure and what’s happening under the hood:

| Concept                | Purpose                                                          | Notes                                |
| ---------------------- | ---------------------------------------------------------------- | ------------------------------------ |
| **LangChain**          | The framework providing model abstractions, chains, memory, etc. | Acts as the “AI engine layer”        |
| **LLM Model Provider** | Your actual LLM backend — OpenAI, Anthropic, etc.                | You’ll use `@langchain/openai`       |
| **modelProvider.ts**   | Centralizes model configuration (temperature, max tokens, etc.)  | Keeps model logic DRY and consistent |
| **Controller & Route** | Handles incoming requests and returns model responses            | Isolated from LangChain logic        |
| **Environment Config** | Secures API keys & model tuning variables                        | `.env` → `config/env.ts`             |

---

## ⚙️ **Implementation Steps**

### **Step 1 — Install Packages**

Run this inside your `server` directory:

```bash
yarn add langchain @langchain/openai dotenv express
yarn add -D @types/express typescript ts-node nodemon
```

---

### **Step 2 — Setup Environment Variables**

Create a `.env` file in the project root:

```bash
OPENAI_API_KEY=your_openai_api_key_here
PORT=5000
```

Then in `src/config/env.ts`:

```ts
import dotenv from 'dotenv'
dotenv.config()

export const config = {
  port: process.env.PORT || 5000,
  openAIApiKey: process.env.OPENAI_API_KEY || '',
}
```

---

### **Step 3 — Create Model Provider**

`src/langchain/config/modelProvider.ts`

This file keeps all LLM-related initialization in one place, so when we upgrade to Agents or RAG, we’ll reuse it.

```ts
import { ChatOpenAI } from '@langchain/openai'
import { config } from '../../config/env'

// Reusable model instance (shared across modules)
export const chatModel = new ChatOpenAI({
  modelName: 'gpt-4o-mini', // lightweight + smart
  temperature: 0.4,
  apiKey: config.openAIApiKey,
})
```

---

### **Step 4 — Create LangChain Controller**

`src/controllers/langchain.controller.ts`

This file defines a simple test endpoint that sends a prompt to the model and returns the response.

```ts
import { Request, Response } from 'express'
import { chatModel } from '../langchain/config/modelProvider'

export const testLLM = async (req: Request, res: Response) => {
  try {
    const userPrompt = req.body.prompt || 'Give me a motivational quote.'

    const response = await chatModel.invoke(userPrompt)

    return res.json({
      success: true,
      prompt: userPrompt,
      output: response.content,
    })
  } catch (error: any) {
    console.error('LLM Test Error:', error)
    return res.status(500).json({ success: false, message: 'LLM test failed' })
  }
}
```

---

### **Step 5 — Create Routes**

`src/routes/langchain.routes.ts`

```ts
import express from 'express'
import { testLLM } from '../controllers/langchain.controller'

const router = express.Router()

router.post('/test', testLLM)

export default router
```

Then register it in your main routes file:

`src/routes/index.ts`

```ts
import express from 'express'
import langchainRoutes from './langchain.routes'

const router = express.Router()

router.use('/langchain', langchainRoutes)
router.get('/health', (_, res) => res.json({ status: 'ok' }))

export default router
```

And import it inside your main `app.ts`:

```ts
import express from 'express'
import { config } from './config/env'
import routes from './routes'

const app = express()
app.use(express.json())
app.use('/api', routes)

app.listen(config.port, () => console.log(`🚀 Server running on port ${config.port}`))
```

---

### **Step 6 — Test Your Setup**

Run your server:

```bash
yarn dev
```

Then test the route using Postman or cURL:

**POST** `http://localhost:5000/api/langchain/test`
**Body:**

```json
{ "prompt": "Summarize the purpose of LangChain in one line" }
```

✅ **Expected Output:**

```json
{
  "success": true,
  "prompt": "Summarize the purpose of LangChain in one line",
  "output": "LangChain connects language models with data and tools for building intelligent applications."
}
```

---

## 🧱 **What We’ve Built**

✅ Configured LangChain + OpenAI
✅ Created a central model provider
✅ Built and tested a modular controller route
✅ Verified working integration between Node → LangChain → LLM
