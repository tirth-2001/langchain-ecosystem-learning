# **6.1.3 â€“ Basic Chains & Endpoints with Steaming Response**

## ğŸ¯ **Goal**

Build your **first LangChain-powered API endpoint** (`/api/ask`) that:

1. Accepts user queries
2. Passes them through a **LangChain chain (Prompt + LLM)**
3. Supports **streaming** responses using **Server-Sent Events (SSE)**
4. Returns data safely even under partial failures

---

## ğŸ“– **Theory**

### ğŸ”¹ 1. What is a Chain in LangChain?

A **Chain** is a modular building block that connects **inputs**, **prompts**, **models**, and **outputs**.

Think of it as a pipeline:

```
User Input â†’ Prompt Template â†’ LLM â†’ Output Parser â†’ Response
```

Common chain types:

- **LLMChain:** simplest form; single prompt + model.
- **Sequential / Router Chains:** combine multiple chains.
- **Custom Chains:** your own logic inside a chain wrapper.

---

### ğŸ”¹ 2. Streaming in LangChain

Normally, `model.invoke()` waits for the _entire_ output.
Streaming allows us to handle tokens as theyâ€™re generated.

LangChainâ€™s `ChatOpenAI` supports streaming via a callback pattern:

```ts
const model = new ChatOpenAI({
  streaming: true,
  callbacks: [
    {
      handleLLMNewToken(token) {
        process.stdout.write(token)
      },
    },
  ],
})
```

We can use this to push tokens to clients over **SSE (Server-Sent Events)**.

---

### ğŸ”¹ 3. What is SSE?

SSE (Server-Sent Events) is a **unidirectional** stream from server â†’ client.

- Protocol: HTTP-based (`Content-Type: text/event-stream`)
- Connection remains open; server sends data chunks with prefix `data: ...\n\n`
- Lightweight vs. WebSocket, perfect for AI token streams

Browser side:

```js
const stream = new EventSource('/api/ask/stream')
stream.onmessage = (event) => console.log(event.data)
```

---

## ğŸ’» **Implementation**

### **File:** `src/langchain/chains/simpleChatChain.ts`

```ts
import { ChatOpenAI } from '@langchain/openai'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'

export const createSimpleChatChain = () => {
  const prompt = ChatPromptTemplate.fromMessages([
    ['system', 'You are a helpful AI assistant. Keep responses concise.'],
    ['human', '{input}'],
  ])

  const model = new ChatOpenAI({
    modelName: 'gpt-4o-mini',
    temperature: 0.7,
    streaming: true, // enables token-wise streaming
  })

  const chain = RunnableSequence.from([prompt, model])
  return chain
}
```

---

### **File:** `src/controllers/langchain.controller.ts`

```ts
import { Request, Response } from 'express'
import { createSimpleChatChain } from '../langchain/chains/simpleChatChain'

export const askController = async (req: Request, res: Response) => {
  const { query } = req.body

  if (!query) {
    return res.status(400).json({ error: "Missing 'query' field in body" })
  }

  try {
    const chain = createSimpleChatChain()

    // Non-streaming fallback (for debugging)
    // const result = await chain.invoke({ input: query });
    // return res.json({ output: result.content });

    // STREAMING: use Server-Sent Events
    res.setHeader('Content-Type', 'text/event-stream')
    res.setHeader('Cache-Control', 'no-cache')
    res.setHeader('Connection', 'keep-alive')

    let buffer = ''

    const response = await chain.stream({ input: query })
    for await (const chunk of response) {
      const content = chunk?.content ?? ''
      buffer += content
      res.write(`data: ${content}\n\n`)
    }

    res.write(`event: end\ndata: ${JSON.stringify({ full: buffer })}\n\n`)
    res.end()
  } catch (err: any) {
    console.error('Error in /api/ask:', err.message)
    res.write(`event: error\ndata: ${JSON.stringify({ error: err.message })}\n\n`)
    res.end()
  }
}
```

---

### **File:** `src/routes/langchain.routes.ts`

```ts
import { Router } from 'express'
import { askController } from '../controllers/langchain.controller'

const router = Router()
router.post('/ask', askController)
export default router
```

---

### **File:** `src/app.ts`

(make sure routes are registered)

```ts
import express from 'express'
import cors from 'cors'
import bodyParser from 'body-parser'
import langchainRoutes from './routes/langchain.routes'

const app = express()
app.use(cors())
app.use(bodyParser.json())

app.use('/api', langchainRoutes)

app.get('/api/health', (req, res) => res.json({ status: 'ok' }))

export default app
```

---

### **Run & Test (Postman / Terminal)**

#### ğŸ”¹ Non-streaming (POST)

```
POST /api/ask
{
  "query": "Explain the difference between RAG and fine-tuning in short."
}
```

Response:

```json
{ "output": "RAG retrieves external data dynamically, while fine-tuning embeds knowledge into the model." }
```

#### ğŸ”¹ Streaming (SSE)

From browser or terminal:

```bash
curl -N -X POST http://localhost:3000/api/ask -H "Content-Type: application/json" -d '{"query":"Summarize LangChain in 1 paragraph"}'
```

Youâ€™ll see tokens streamed in real time.

---

## ğŸ”§ **Enhancements (Optional)**

1. **Session Memory**

   - Integrate `BufferMemory` or `ConversationSummaryMemory` for chat continuity.

2. **Prompt Customization**

   - Inject role-based context (e.g., â€œYouâ€™re an HR assistantâ€¦â€)

3. **Error Recovery**

   - Auto-close SSE on exceptions; send `event: error`.

4. **Frontend streaming**

   - Later in Stage 6.2, weâ€™ll consume this stream in React with `EventSource`.

---

## âœ… **Outcome**

After **6.1.3**, you now have:

- A working `/api/ask` endpoint.
- Real-time token streaming using SSE.
- Reusable LangChain chain logic (`simpleChatChain`).
- Ready foundation for task orchestration in next stage.
