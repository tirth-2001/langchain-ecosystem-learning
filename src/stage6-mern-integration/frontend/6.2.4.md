## âš¡ **Stage 6.2.4 â€“ Streaming (SSE) Integration**

### ğŸ¯ **Goal**

Implement a **reactive streaming layer** that connects to `/api/stream` (our backend SSE endpoint from Stage 6.1.3) and progressively displays model responses.

---

## ğŸ§  **Conceptual Overview**

LangChain and OpenAI both emit responses **incrementally** (token-by-token).
On the backend, weâ€™ve already sent chunks like:

```ts
data: {"chunk": "Hello "}
data: {"chunk": "world!"}
data: [DONE]
```

On the frontend, weâ€™ll **consume this EventSource stream** and update the chat progressively â€” without overlapping or losing tokens.

---

## ğŸ—ï¸ **Implementation Plan**

Weâ€™ll build this in 3 key layers:

### **1ï¸âƒ£ Utility Hook â€“ `useStreamLLM`**

This hook connects to the SSE endpoint, listens to events, and updates UI progressively.

**File:** `src/hooks/useStreamLLM.ts`

```ts
import { useState, useRef, useCallback } from 'react'

interface StreamOptions {
  prompt: string
  onComplete?: (finalText: string) => void
  onError?: (error: string) => void
}

export function useStreamLLM() {
  const [isStreaming, setIsStreaming] = useState(false)
  const [response, setResponse] = useState('')
  const eventSourceRef = useRef<EventSource | null>(null)

  const startStream = useCallback(
    async ({ prompt, onComplete, onError }: StreamOptions) => {
      setIsStreaming(true)
      setResponse('')

      try {
        const url = new URL('/api/stream', import.meta.env.VITE_API_BASE_URL)
        url.searchParams.append('prompt', prompt)

        const eventSource = new EventSource(url.toString())
        eventSourceRef.current = eventSource

        eventSource.onmessage = (event) => {
          if (event.data === '[DONE]') {
            eventSource.close()
            setIsStreaming(false)
            onComplete?.(response)
            return
          }

          try {
            const data = JSON.parse(event.data)
            if (data.chunk) {
              setResponse((prev) => prev + data.chunk)
            }
          } catch (err) {
            console.error('Malformed SSE data:', event.data)
          }
        }

        eventSource.onerror = (err) => {
          console.error('SSE error', err)
          onError?.('Streaming error occurred')
          eventSource.close()
          setIsStreaming(false)
        }
      } catch (err: any) {
        console.error('Stream init error', err)
        onError?.(err.message)
        setIsStreaming(false)
      }
    },
    [response],
  )

  const stopStream = useCallback(() => {
    if (eventSourceRef.current) {
      eventSourceRef.current.close()
      setIsStreaming(false)
    }
  }, [])

  return { response, isStreaming, startStream, stopStream }
}
```

âœ… **Key highlights:**

- Uses `EventSource` for native SSE.
- Appends chunks progressively.
- Closes connection cleanly on `[DONE]` or error.
- Avoids duplicate chunk merging or loss.

---

### **2ï¸âƒ£ Integrate With Context**

Extend our `LLMContext` to handle both normal + streaming modes.

**File:** `src/context/LLMContext.tsx`

```tsx
import { createContext, useContext, useState } from 'react'
import { useStreamLLM } from '../hooks/useStreamLLM'
import { askLLM } from '../api/llmApi'

interface LLMContextValue {
  response: string
  isStreaming: boolean
  ask: (prompt: string, streaming?: boolean) => Promise<void>
  stop: () => void
}

const LLMContext = createContext<LLMContextValue | null>(null)

export const LLMProvider = ({ children }: { children: React.ReactNode }) => {
  const [response, setResponse] = useState('')
  const { response: streamResp, isStreaming, startStream, stopStream } = useStreamLLM()

  const ask = async (prompt: string, streaming = false) => {
    setResponse('')
    if (streaming) {
      startStream({
        prompt,
        onComplete: (final) => setResponse(final),
      })
    } else {
      const res = await askLLM(prompt)
      setResponse(res.output)
    }
  }

  return (
    <LLMContext.Provider value={{ response: streaming ? streamResp : response, isStreaming, ask, stop: stopStream }}>
      {children}
    </LLMContext.Provider>
  )
}

export const useLLM = () => useContext(LLMContext)!
```

âœ… **Now the Context supports both REST and SSE modes!**

---

### **3ï¸âƒ£ Example UI Component â€“ `StreamChat.tsx`**

**File:** `src/components/StreamChat.tsx`

```tsx
import { useState, useEffect, useRef } from 'react'
import { useLLM } from '../context/LLMContext'

export const StreamChat = () => {
  const [input, setInput] = useState('')
  const { response, ask, isStreaming } = useLLM()
  const containerRef = useRef<HTMLDivElement | null>(null)

  // Auto-scroll on new chunks
  useEffect(() => {
    if (containerRef.current) {
      containerRef.current.scrollTop = containerRef.current.scrollHeight
    }
  }, [response])

  return (
    <div className="max-w-xl mx-auto p-4">
      <h2 className="text-xl font-semibold mb-2">ğŸ’¬ Streaming Chat</h2>

      <div
        ref={containerRef}
        className="border rounded-lg p-3 h-64 overflow-y-auto bg-gray-50 font-mono text-sm whitespace-pre-wrap"
      >
        {response || (isStreaming ? 'Streaming...' : 'No response yet')}
      </div>

      <div className="flex gap-2 mt-4">
        <input
          className="grow border px-3 py-2 rounded"
          placeholder="Enter your prompt"
          value={input}
          onChange={(e) => setInput(e.target.value)}
        />
        <button
          className="bg-blue-500 text-white px-4 py-2 rounded"
          disabled={isStreaming}
          onClick={() => ask(input, true)} // Streaming mode ON
        >
          {isStreaming ? 'Streaming...' : 'Send'}
        </button>
      </div>
    </div>
  )
}
```

âœ… **Features:**

- Realtime progressive response update
- Auto-scroll to bottom
- Supports both â€œstreamingâ€ and â€œRESTâ€ ask modes

---

## ğŸ§© **Optional Enhancements**

| Feature                          | Approach                                                                         |
| -------------------------------- | -------------------------------------------------------------------------------- |
| **Markdown Rendering**           | Use `react-markdown` and pass `response` to it for formatted output.             |
| **Stop Generation Button**       | Hook into `stop()` from context.                                                 |
| **Rich content (tables/images)** | Add markdown plugins for GFM (`remark-gfm`) to support tables, code blocks, etc. |
| **Multiple messages / history**  | Extend context to store an array of `{ role, content }` messages.                |

---

## ğŸ§  **Outcome**

By the end of this subsection:

- âœ… Fully functional **streaming chat** experience
- âœ… Smooth real-time UI updates (no flicker / overlap)
- âœ… Context-powered integration with fallback to REST
- âœ… Ready for later â€œChat UI + Message Historyâ€ stage
