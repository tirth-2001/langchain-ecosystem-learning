# üß© **Stage 6.2.3 ‚Äî Context & State Management Layer**

---

## üéØ **Goal**

Build a **scalable state management architecture** using React Context + Hooks that:

- Keeps **LangChain/LLM responses** and **Tasks** globally accessible
- Supports both **standard and streaming** data updates
- Minimizes prop-drilling
- Prepares frontend for agent + RAG extensions later

---

## üß† **Theory Overview**

LangChain and agentic apps typically have 2 main data domains:

| Context         | Purpose                                          | Example Data                             |
| --------------- | ------------------------------------------------ | ---------------------------------------- |
| **TaskContext** | Manage CRUD state of AI tasks                    | task list, selected task, running status |
| **LLMContext**  | Handle conversations, streaming, and chat memory | messages, partial output, loading        |

We‚Äôll implement both contexts, each with its own provider and hooks:

- üü¢ `useTasks()` ‚Üí CRUD and trigger LangChain executions
- üü£ `useLLM()` ‚Üí ask questions, stream responses, clear history

This structure ensures clean separation between **business logic** and **UI components**.

---

## üìÇ **Folder Structure Update**

```
client/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ context/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TaskContext.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LLMContext.tsx
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useTasks.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useLLM.ts
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
```

---

## ‚öôÔ∏è **Implementation**

---

### **1Ô∏è‚É£ TaskContext ‚Äî Global Task Store**

#### `src/context/TaskContext.tsx`

```tsx
import { createContext, useContext, useState, ReactNode } from 'react'
import { Task } from '../api/types'
import { createTask, getAllTasks } from '../api/endpoints/tasks.api'
import { handleApiError } from '../utils/errorHandler'

interface TaskContextType {
  tasks: Task[]
  loading: boolean
  error: string | null
  fetchTasks: () => Promise<void>
  addTask: (title: string, description: string) => Promise<void>
}

const TaskContext = createContext<TaskContextType | undefined>(undefined)

export const TaskProvider = ({ children }: { children: ReactNode }) => {
  const [tasks, setTasks] = useState<Task[]>([])
  const [loading, setLoading] = useState(false)
  const [error, setError] = useState<string | null>(null)

  const fetchTasks = async () => {
    setLoading(true)
    try {
      const res = await getAllTasks()
      setTasks(res.data)
    } catch (err) {
      setError(handleApiError(err))
    } finally {
      setLoading(false)
    }
  }

  const addTask = async (title: string, description: string) => {
    setLoading(true)
    try {
      const res = await createTask({ title, description })
      setTasks((prev) => [...prev, res.data])
    } catch (err) {
      setError(handleApiError(err))
    } finally {
      setLoading(false)
    }
  }

  return <TaskContext.Provider value={{ tasks, loading, error, fetchTasks, addTask }}>{children}</TaskContext.Provider>
}

export const useTaskContext = () => {
  const ctx = useContext(TaskContext)
  if (!ctx) throw new Error('useTaskContext must be used within TaskProvider')
  return ctx
}
```

---

### **2Ô∏è‚É£ LLMContext ‚Äî Stream & Chat State**

#### `src/context/LLMContext.tsx`

```tsx
import { createContext, useContext, useState, ReactNode } from 'react'

interface Message {
  role: 'user' | 'assistant'
  content: string
}

interface LLMContextType {
  messages: Message[]
  loading: boolean
  askQuestion: (prompt: string) => Promise<void>
  clearChat: () => void
}

const LLMContext = createContext<LLMContextType | undefined>(undefined)

export const LLMProvider = ({ children }: { children: ReactNode }) => {
  const [messages, setMessages] = useState<Message[]>([])
  const [loading, setLoading] = useState(false)

  // Basic non-streaming ask call
  const askQuestion = async (prompt: string) => {
    setLoading(true)
    try {
      setMessages((prev) => [...prev, { role: 'user', content: prompt }])

      const res = await fetch(`${import.meta.env.VITE_API_BASE_URL}/ask`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt }),
      })
      const data = await res.json()

      setMessages((prev) => [...prev, { role: 'assistant', content: data.data.reply }])
    } catch (error) {
      setMessages((prev) => [...prev, { role: 'assistant', content: '‚ö†Ô∏è Error reaching the model.' }])
    } finally {
      setLoading(false)
    }
  }

  const clearChat = () => setMessages([])

  return <LLMContext.Provider value={{ messages, loading, askQuestion, clearChat }}>{children}</LLMContext.Provider>
}

export const useLLMContext = () => {
  const ctx = useContext(LLMContext)
  if (!ctx) throw new Error('useLLMContext must be used within LLMProvider')
  return ctx
}
```

---

### **3Ô∏è‚É£ Hook Aliases (Optional Layer for Cleaner Imports)**

`src/hooks/useTasks.ts`

```ts
export { useTaskContext as useTasks } from '../context/TaskContext'
```

`src/hooks/useLLM.ts`

```ts
export { useLLMContext as useLLM } from '../context/LLMContext'
```

Now, instead of importing from `context/...`, we can do:

```ts
import { useTasks } from '../hooks/useTasks'
import { useLLM } from '../hooks/useLLM'
```

---

### **4Ô∏è‚É£ Wrap Providers in Root App**

`src/main.tsx`

```tsx
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App'
import './styles/index.css'
import { TaskProvider } from './context/TaskContext'
import { LLMProvider } from './context/LLMContext'

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <TaskProvider>
      <LLMProvider>
        <App />
      </LLMProvider>
    </TaskProvider>
  </React.StrictMode>,
)
```

---

### **5Ô∏è‚É£ Test Example Component**

`src/components/ChatDemo.tsx`

```tsx
import { useState } from 'react'
import { useLLM } from '../hooks/useLLM'

export default function ChatDemo() {
  const { messages, askQuestion, clearChat, loading } = useLLM()
  const [input, setInput] = useState('')

  const handleSubmit = async () => {
    if (!input.trim()) return
    await askQuestion(input)
    setInput('')
  }

  return (
    <div className="max-w-2xl mx-auto mt-6 p-4 bg-white rounded-2xl shadow">
      <h2 className="text-xl font-bold mb-2 text-center">üí¨ Chat Demo</h2>

      <div className="h-64 overflow-y-auto border p-2 rounded-md bg-gray-50 mb-3">
        {messages.map((m, i) => (
          <p key={i} className={`mb-1 ${m.role === 'user' ? 'text-blue-700 font-medium' : 'text-gray-800'}`}>
            <b>{m.role === 'user' ? 'You:' : 'AI:'}</b> {m.content}
          </p>
        ))}
      </div>

      <div className="flex gap-2">
        <input
          className="flex-1 border border-gray-300 rounded-lg p-2"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="Ask something..."
        />
        <button onClick={handleSubmit} disabled={loading} className="bg-blue-600 text-white px-4 rounded-lg">
          Send
        </button>
        <button onClick={clearChat} className="bg-gray-300 px-4 rounded-lg">
          Clear
        </button>
      </div>
    </div>
  )
}
```

---

## ‚úÖ **Checkpoint Summary**

| Layer              | Responsibility              | Status |
| ------------------ | --------------------------- | ------ |
| **TaskContext**    | CRUD & state for tasks      | ‚úÖ     |
| **LLMContext**     | Chat, streaming-ready       | ‚úÖ     |
| **Hooks**          | Easy access from components | ‚úÖ     |
| **Provider setup** | Wrapped around app          | ‚úÖ     |
| **Chat demo**      | Basic LLM conversation      | ‚úÖ     |
